{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Library Import \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from datetime import datetime\n",
    "\n",
    "from MAIN.utils import *\n",
    "from MAIN.train import *\n",
    "import MAIN.preprocess_functions\n",
    "from MAIN.GNN_MME import GCN_MME , GSage_MME , GAT_MME\n",
    "\n",
    "from Modules.PNetTorch.MAIN.reactome import ReactomeNetwork\n",
    "from Modules.PNetTorch.MAIN.Pnet import MaskedLinear , PNET\n",
    "from Modules.PNetTorch.MAIN.utils import numpy_array_to_one_hot, get_gpu_memory\n",
    "from Modules.PNetTorch.MAIN.interpret import interpret , evaluate_interpret_save , visualize_importances\n",
    "\n",
    "import dgl\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"Finished Library Import \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the merge operation setup\n",
    "def merge_dfs(left_df, right_df):\n",
    "    # Merging on 'key' and expanding with 'how=outer' to include all records\n",
    "    return pd.merge(left_df, right_df, left_index=True, right_index=True, how='outer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "no_cuda=True\n",
    "output=\"训练\\\\result\\\\\"\n",
    "# Map model names to class objects\n",
    "model_mapping = {\n",
    "    \"GCN\": GCN_MME,\n",
    "    \"GSage\": GSage_MME,\n",
    "    'GAT': GAT_MME\n",
    "}\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Check if output directory exists, if not create it\n",
    "if not os.path.exists(output) : \n",
    "    os.makedirs(output, exist_ok=True)\n",
    "    \n",
    "# Specify the device to use\n",
    "device = torch.device('cpu' if no_cuda else 'cuda') # Get GPU device name, else use CPU\n",
    "print(\"Using %s device\" % device)\n",
    "#get_gpu_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modality: path, Shape: (1551, 343)\n",
      "Modality: cell, Shape: (1551, 86)\n"
     ]
    }
   ],
   "source": [
    "input=\"训练\"\n",
    "modalities=['path', 'cell']\n",
    "target='diagnosis'\n",
    "index_col='sample_id'\n",
    "label_file=\"labels.csv\"\n",
    "# Load data and metadata\n",
    "datModalities , meta = data_parsing(input , modalities , target , index_col,label_file)\n",
    "for mod, expr in datModalities.items():\n",
    "    print(f\"Modality: {mod}, Shape: {expr.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpret_feat=False\n",
    "pnet=False\n",
    "if interpret_feat : \n",
    "    features = {}\n",
    "    for i , mod in enumerate(datModalities) : \n",
    "        features[i] = list(datModalities[mod].columns)\n",
    "\n",
    "if pnet : \n",
    "    # List of genes of interest in PNet (keep to less than 1000 for big models)\n",
    "    genes = pd.read_csv(f'{input}/../ext_data/genelist.txt', header=0)\n",
    "\n",
    "    # Build network to obtain gene and pathway relationships\n",
    "    net = ReactomeNetwork(genes_of_interest=np.unique(list(genes['genes'].values)) , n_levels=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = meta.loc[sorted(meta.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将 diagnosis 列转换为分类数据类型\n",
    "diagnosis_series = meta['diagnosis'].astype('category')\n",
    "\n",
    "# 获取独热编码\n",
    "label = F.one_hot(torch.Tensor(diagnosis_series.cat.codes).to(torch.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "MME_input_shapes = [ datModalities[mod].shape[1] for mod in datModalities]\n",
    "h = reduce(merge_dfs , list(datModalities.values()))\n",
    "h = h.loc[sorted(h.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 1551\n",
      "Number of edges: 13429\n",
      "Graph(num_nodes=1551, num_edges=13429,\n",
      "      ndata_schemes={'feat': Scheme(shape=(429,), dtype=torch.float32), 'label': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={})\n"
     ]
    }
   ],
   "source": [
    "# 读取 CSV 文件\n",
    "csv_file = '训练\\\\snf_graph.csv'\n",
    "df = pd.read_csv(csv_file, na_values=['NA'])\n",
    "\n",
    "# 创建一个有向图\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# 添加边到图中\n",
    "for index, row in df.iterrows():\n",
    "    G.add_edge(row['from'], row['to'], \n",
    "               from_color=row['from_frame.color'], \n",
    "               from_name=row['from_name'], \n",
    "               from_class=row['from_class'], \n",
    "               from_vertex_color=row['from_vertex.frame.color'],\n",
    "               to_color=row['to_frame.color'], \n",
    "               to_name=row['to_name'], \n",
    "               to_class=row['to_class'], \n",
    "               to_vertex_color=row['to_vertex.frame.color'])\n",
    "\n",
    "# 将 NetworkX 图转换为 DGL 图\n",
    "g = dgl.from_networkx(G)\n",
    "\n",
    "# 打印图的一些基本信息\n",
    "print(\"Number of nodes:\", g.number_of_nodes())\n",
    "print(\"Number of edges:\", g.number_of_edges())\n",
    "\n",
    "# 这里可以为节点和边添加特征，如果需要的话\n",
    "# 例如，可以为节点添加一个特征张量\n",
    "g.ndata['feat'] = torch.Tensor(h.to_numpy())\n",
    "#node_features = torch.zeros(g.number_of_nodes(), 3)  # 假设每个节点有3个特征\n",
    "#g.ndata['feat'] = node_features\n",
    "\n",
    "# 如果需要为边添加特征\n",
    "#edge_features = torch.zeros(g.number_of_edges(), 4)  # 假设每条边有4个特征\n",
    "#g.edata['feat'] = edge_features\n",
    "g.ndata['label'] = torch.tensor(meta['diagnosis'], dtype=torch.int64)\n",
    "# 输出图的结构\n",
    "print(g)\n",
    "g = dgl.add_self_loop(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StratifiedKFold(n_splits=5, random_state=None, shuffle=True)\n"
     ]
    }
   ],
   "source": [
    "no_shuffle=False\n",
    "n_splits=5\n",
    "# Generate K Fold splits\n",
    "if no_shuffle : \n",
    "    skf = StratifiedKFold(n_splits=n_splits , shuffle=False) \n",
    "else :\n",
    "    skf = StratifiedKFold(n_splits=n_splits , shuffle=True) \n",
    "\n",
    "print(skf)\n",
    "\n",
    "output_metrics = []\n",
    "test_logits = []\n",
    "test_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#`python MOGDx.py -i \"/raw_data/raw_BRCA\" -o \"./Output/BRCA/\"  -snf \"mRNA_miRNA_graph.csv\" --n-splits 5 -ld 32 16 --target \"paper_BRCA_Subtype_PAM50\" --index-col \"patient\" --epochs 2000 --lr 0.001 --h-feats 64 --decoder-dim 64`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim=[32,16]\n",
    "model=\"GCN\"\n",
    "decoder_dim=64\n",
    "h_feats= [64]\n",
    "epochs=2000\n",
    "lr=0.001\n",
    "patience=100\n",
    "output=\"训练\\\\result\\\\\"\n",
    "interpret_feat=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN_MME(\n",
      "  (encoder_dims): ModuleList(\n",
      "    (0): Encoder(\n",
      "      (encoder): ModuleList(\n",
      "        (0): Linear(in_features=343, out_features=500, bias=True)\n",
      "        (1): Linear(in_features=500, out_features=16, bias=True)\n",
      "      )\n",
      "      (norm): ModuleList(\n",
      "        (0): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (decoder): Sequential(\n",
      "        (0): Linear(in_features=16, out_features=32, bias=True)\n",
      "      )\n",
      "      (drop): Dropout(p=0.5, inplace=False)\n",
      "    )\n",
      "    (1): Encoder(\n",
      "      (encoder): ModuleList(\n",
      "        (0): Linear(in_features=86, out_features=500, bias=True)\n",
      "        (1): Linear(in_features=500, out_features=32, bias=True)\n",
      "      )\n",
      "      (norm): ModuleList(\n",
      "        (0): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (decoder): Sequential(\n",
      "        (0): Linear(in_features=32, out_features=32, bias=True)\n",
      "      )\n",
      "      (drop): Dropout(p=0.5, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (gnnlayers): ModuleList(\n",
      "    (0): GraphConv(in=32, out=16, normalization=both, activation=None)\n",
      "    (1): GraphConv(in=16, out=4, normalization=both, activation=None)\n",
      "  )\n",
      "  (batch_norms): ModuleList(\n",
      "    (0): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "Graph(num_nodes=1551, num_edges=14980,\n",
      "      ndata_schemes={'feat': Scheme(shape=(429,), dtype=torch.float32), 'label': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={})\n",
      "Epoch 00000 | Loss 1.9272 | Train Acc. 0.2760 | \n",
      "Epoch 00005 | Loss 1.3582 | Train Acc. 0.4287 | \n",
      "Epoch 00010 | Loss 1.3280 | Train Acc. 0.4554 | \n",
      "Epoch 00015 | Loss 1.2415 | Train Acc. 0.5160 | \n",
      "Epoch 00020 | Loss 1.2162 | Train Acc. 0.5401 | \n",
      "Epoch 00025 | Loss 1.2552 | Train Acc. 0.5143 | \n",
      "Epoch 00030 | Loss 1.2356 | Train Acc. 0.5079 | \n",
      "Epoch 00035 | Loss 1.1425 | Train Acc. 0.5363 | \n",
      "Epoch 00040 | Loss 1.2089 | Train Acc. 0.5345 | \n",
      "Epoch 00045 | Loss 1.1348 | Train Acc. 0.5652 | \n",
      "Epoch 00050 | Loss 1.1236 | Train Acc. 0.5735 | \n",
      "Epoch 00055 | Loss 1.1727 | Train Acc. 0.5414 | \n",
      "Epoch 00060 | Loss 1.1607 | Train Acc. 0.5649 | \n",
      "Epoch 00065 | Loss 1.1699 | Train Acc. 0.5644 | \n",
      "Epoch 00070 | Loss 1.1336 | Train Acc. 0.5398 | \n",
      "Epoch 00075 | Loss 1.1853 | Train Acc. 0.5552 | \n",
      "Epoch 00080 | Loss 1.0491 | Train Acc. 0.5682 | \n",
      "Epoch 00085 | Loss 1.1776 | Train Acc. 0.5431 | \n",
      "Epoch 00090 | Loss 1.1132 | Train Acc. 0.5979 | \n",
      "Epoch 00095 | Loss 1.1508 | Train Acc. 0.5555 | \n",
      "Epoch 00100 | Loss 1.1213 | Train Acc. 0.5784 | \n",
      "Epoch 00105 | Loss 1.1525 | Train Acc. 0.5534 | \n",
      "Epoch 00110 | Loss 1.1035 | Train Acc. 0.5739 | \n",
      "Epoch 00115 | Loss 1.1388 | Train Acc. 0.5678 | \n",
      "Epoch 00120 | Loss 1.1007 | Train Acc. 0.5617 | \n",
      "Epoch 00125 | Loss 1.1572 | Train Acc. 0.5529 | \n",
      "Epoch 00130 | Loss 1.1775 | Train Acc. 0.5355 | \n",
      "Epoch 00135 | Loss 1.0851 | Train Acc. 0.5807 | \n",
      "Epoch 00140 | Loss 1.0835 | Train Acc. 0.5889 | \n",
      "Epoch 00145 | Loss 1.1029 | Train Acc. 0.5611 | \n",
      "Epoch 00150 | Loss 1.1839 | Train Acc. 0.5492 | \n",
      "Epoch 00155 | Loss 1.0811 | Train Acc. 0.5849 | \n",
      "Epoch 00160 | Loss 1.1039 | Train Acc. 0.5690 | \n",
      "Epoch 00165 | Loss 1.1420 | Train Acc. 0.5664 | \n",
      "Epoch 00170 | Loss 1.1593 | Train Acc. 0.5699 | \n",
      "Epoch 00175 | Loss 1.1420 | Train Acc. 0.5554 | \n",
      "Epoch 00180 | Loss 1.1578 | Train Acc. 0.5411 | \n",
      "Epoch 00185 | Loss 1.1126 | Train Acc. 0.5620 | \n",
      "Epoch 00190 | Loss 1.0604 | Train Acc. 0.5723 | \n",
      "Epoch 00195 | Loss 1.1091 | Train Acc. 0.5693 | \n",
      "Epoch 00200 | Loss 1.1533 | Train Acc. 0.5495 | \n",
      "Epoch 00205 | Loss 1.1292 | Train Acc. 0.5531 | \n",
      "Epoch 00210 | Loss 1.1054 | Train Acc. 0.5743 | \n",
      "Epoch 00215 | Loss 1.0643 | Train Acc. 0.5919 | \n",
      "Epoch 00220 | Loss 1.1495 | Train Acc. 0.5578 | \n",
      "Epoch 00225 | Loss 1.1132 | Train Acc. 0.5649 | \n",
      "Epoch 00230 | Loss 1.0880 | Train Acc. 0.5665 | \n",
      "Epoch 00235 | Loss 1.0640 | Train Acc. 0.5974 | \n",
      "Epoch 00240 | Loss 1.1494 | Train Acc. 0.5583 | \n",
      "Early stopping! No improvement for 100 consecutive epochs.\n",
      "<dgl.dataloading.dataloader.DataLoader object at 0x000002576A1A28C0>\n",
      "Fold : 1 | Test Accuracy = 0.5949 | F1 = 0.5116 \n",
      "Clearing gpu memory\n",
      "GCN_MME(\n",
      "  (encoder_dims): ModuleList(\n",
      "    (0): Encoder(\n",
      "      (encoder): ModuleList(\n",
      "        (0): Linear(in_features=343, out_features=500, bias=True)\n",
      "        (1): Linear(in_features=500, out_features=16, bias=True)\n",
      "      )\n",
      "      (norm): ModuleList(\n",
      "        (0): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (decoder): Sequential(\n",
      "        (0): Linear(in_features=16, out_features=32, bias=True)\n",
      "      )\n",
      "      (drop): Dropout(p=0.5, inplace=False)\n",
      "    )\n",
      "    (1): Encoder(\n",
      "      (encoder): ModuleList(\n",
      "        (0): Linear(in_features=86, out_features=500, bias=True)\n",
      "        (1): Linear(in_features=500, out_features=32, bias=True)\n",
      "      )\n",
      "      (norm): ModuleList(\n",
      "        (0): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (decoder): Sequential(\n",
      "        (0): Linear(in_features=32, out_features=32, bias=True)\n",
      "      )\n",
      "      (drop): Dropout(p=0.5, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (gnnlayers): ModuleList(\n",
      "    (0): GraphConv(in=32, out=16, normalization=both, activation=None)\n",
      "    (1): GraphConv(in=16, out=4, normalization=both, activation=None)\n",
      "  )\n",
      "  (batch_norms): ModuleList(\n",
      "    (0): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "Graph(num_nodes=1551, num_edges=14980,\n",
      "      ndata_schemes={'feat': Scheme(shape=(429,), dtype=torch.float32), 'label': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={})\n",
      "Epoch 00000 | Loss 1.5892 | Train Acc. 0.2906 | \n",
      "Epoch 00005 | Loss 1.3039 | Train Acc. 0.5011 | \n",
      "Epoch 00010 | Loss 1.2332 | Train Acc. 0.5284 | \n",
      "Epoch 00015 | Loss 1.2875 | Train Acc. 0.4806 | \n",
      "Epoch 00020 | Loss 1.2624 | Train Acc. 0.5179 | \n",
      "Epoch 00025 | Loss 1.2003 | Train Acc. 0.5428 | \n",
      "Epoch 00030 | Loss 1.1808 | Train Acc. 0.5387 | \n",
      "Epoch 00035 | Loss 1.2049 | Train Acc. 0.5527 | \n",
      "Epoch 00040 | Loss 1.2017 | Train Acc. 0.5507 | \n",
      "Epoch 00045 | Loss 1.1099 | Train Acc. 0.5592 | \n",
      "Epoch 00050 | Loss 1.1969 | Train Acc. 0.5683 | \n",
      "Epoch 00055 | Loss 1.1253 | Train Acc. 0.5772 | \n",
      "Epoch 00060 | Loss 1.1329 | Train Acc. 0.5677 | \n",
      "Epoch 00065 | Loss 1.0818 | Train Acc. 0.5796 | \n",
      "Epoch 00070 | Loss 1.1652 | Train Acc. 0.5573 | \n",
      "Epoch 00075 | Loss 1.1693 | Train Acc. 0.5882 | \n",
      "Epoch 00080 | Loss 1.0886 | Train Acc. 0.5567 | \n",
      "Epoch 00085 | Loss 1.1480 | Train Acc. 0.5668 | \n",
      "Epoch 00090 | Loss 1.1601 | Train Acc. 0.5450 | \n",
      "Epoch 00095 | Loss 1.1278 | Train Acc. 0.5410 | \n",
      "Epoch 00100 | Loss 1.1694 | Train Acc. 0.5481 | \n",
      "Epoch 00105 | Loss 1.1128 | Train Acc. 0.5699 | \n",
      "Epoch 00110 | Loss 1.0607 | Train Acc. 0.5648 | \n",
      "Epoch 00115 | Loss 1.1338 | Train Acc. 0.5622 | \n",
      "Epoch 00120 | Loss 1.1077 | Train Acc. 0.5644 | \n",
      "Epoch 00125 | Loss 1.1386 | Train Acc. 0.5893 | \n",
      "Epoch 00130 | Loss 1.2028 | Train Acc. 0.5556 | \n",
      "Epoch 00135 | Loss 1.0270 | Train Acc. 0.6151 | \n",
      "Epoch 00140 | Loss 1.0893 | Train Acc. 0.5924 | \n",
      "Epoch 00145 | Loss 1.1196 | Train Acc. 0.5749 | \n",
      "Epoch 00150 | Loss 1.1354 | Train Acc. 0.5671 | \n",
      "Epoch 00155 | Loss 1.1148 | Train Acc. 0.5428 | \n",
      "Epoch 00160 | Loss 1.0793 | Train Acc. 0.5928 | \n",
      "Epoch 00165 | Loss 1.1323 | Train Acc. 0.5634 | \n",
      "Epoch 00170 | Loss 1.1502 | Train Acc. 0.5762 | \n",
      "Epoch 00175 | Loss 1.1118 | Train Acc. 0.5878 | \n",
      "Epoch 00180 | Loss 1.1340 | Train Acc. 0.5563 | \n",
      "Epoch 00185 | Loss 1.0958 | Train Acc. 0.5707 | \n",
      "Epoch 00190 | Loss 1.0656 | Train Acc. 0.5909 | \n",
      "Epoch 00195 | Loss 1.1165 | Train Acc. 0.5422 | \n",
      "Epoch 00200 | Loss 1.1422 | Train Acc. 0.5757 | \n",
      "Epoch 00205 | Loss 1.0805 | Train Acc. 0.6038 | \n",
      "Epoch 00210 | Loss 1.0788 | Train Acc. 0.6009 | \n",
      "Epoch 00215 | Loss 1.1484 | Train Acc. 0.5711 | \n",
      "Epoch 00220 | Loss 1.0808 | Train Acc. 0.5867 | \n",
      "Early stopping! No improvement for 100 consecutive epochs.\n",
      "<dgl.dataloading.dataloader.DataLoader object at 0x0000025767FABB50>\n",
      "Fold : 2 | Test Accuracy = 0.6258 | F1 = 0.5407 \n",
      "Clearing gpu memory\n",
      "GCN_MME(\n",
      "  (encoder_dims): ModuleList(\n",
      "    (0): Encoder(\n",
      "      (encoder): ModuleList(\n",
      "        (0): Linear(in_features=343, out_features=500, bias=True)\n",
      "        (1): Linear(in_features=500, out_features=16, bias=True)\n",
      "      )\n",
      "      (norm): ModuleList(\n",
      "        (0): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (decoder): Sequential(\n",
      "        (0): Linear(in_features=16, out_features=32, bias=True)\n",
      "      )\n",
      "      (drop): Dropout(p=0.5, inplace=False)\n",
      "    )\n",
      "    (1): Encoder(\n",
      "      (encoder): ModuleList(\n",
      "        (0): Linear(in_features=86, out_features=500, bias=True)\n",
      "        (1): Linear(in_features=500, out_features=32, bias=True)\n",
      "      )\n",
      "      (norm): ModuleList(\n",
      "        (0): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (decoder): Sequential(\n",
      "        (0): Linear(in_features=32, out_features=32, bias=True)\n",
      "      )\n",
      "      (drop): Dropout(p=0.5, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (gnnlayers): ModuleList(\n",
      "    (0): GraphConv(in=32, out=16, normalization=both, activation=None)\n",
      "    (1): GraphConv(in=16, out=4, normalization=both, activation=None)\n",
      "  )\n",
      "  (batch_norms): ModuleList(\n",
      "    (0): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "Graph(num_nodes=1551, num_edges=14980,\n",
      "      ndata_schemes={'feat': Scheme(shape=(429,), dtype=torch.float32), 'label': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={})\n",
      "Epoch 00000 | Loss 1.9121 | Train Acc. 0.2669 | \n",
      "Epoch 00005 | Loss 1.4262 | Train Acc. 0.4050 | \n",
      "Epoch 00010 | Loss 1.4208 | Train Acc. 0.4317 | \n",
      "Epoch 00015 | Loss 1.2702 | Train Acc. 0.4940 | \n",
      "Epoch 00020 | Loss 1.2951 | Train Acc. 0.4791 | \n",
      "Epoch 00025 | Loss 1.2637 | Train Acc. 0.5014 | \n",
      "Epoch 00030 | Loss 1.2464 | Train Acc. 0.4740 | \n",
      "Epoch 00035 | Loss 1.2437 | Train Acc. 0.5023 | \n",
      "Epoch 00040 | Loss 1.2266 | Train Acc. 0.4944 | \n",
      "Epoch 00045 | Loss 1.1174 | Train Acc. 0.5391 | \n",
      "Epoch 00050 | Loss 1.2118 | Train Acc. 0.5239 | \n",
      "Epoch 00055 | Loss 1.2050 | Train Acc. 0.5340 | \n",
      "Epoch 00060 | Loss 1.1298 | Train Acc. 0.5303 | \n",
      "Epoch 00065 | Loss 1.1038 | Train Acc. 0.5700 | \n",
      "Epoch 00070 | Loss 1.1723 | Train Acc. 0.5332 | \n",
      "Epoch 00075 | Loss 1.1856 | Train Acc. 0.4974 | \n",
      "Epoch 00080 | Loss 1.1763 | Train Acc. 0.5550 | \n",
      "Epoch 00085 | Loss 1.0703 | Train Acc. 0.5629 | \n",
      "Epoch 00090 | Loss 1.1174 | Train Acc. 0.5702 | \n",
      "Epoch 00095 | Loss 1.1194 | Train Acc. 0.5049 | \n",
      "Epoch 00100 | Loss 1.1342 | Train Acc. 0.5480 | \n",
      "Epoch 00105 | Loss 1.1756 | Train Acc. 0.5499 | \n",
      "Epoch 00110 | Loss 1.1276 | Train Acc. 0.5538 | \n",
      "Epoch 00115 | Loss 1.1428 | Train Acc. 0.5355 | \n",
      "Epoch 00120 | Loss 1.1694 | Train Acc. 0.5358 | \n",
      "Epoch 00125 | Loss 1.1644 | Train Acc. 0.5255 | \n",
      "Epoch 00130 | Loss 1.1375 | Train Acc. 0.5584 | \n",
      "Epoch 00135 | Loss 1.0865 | Train Acc. 0.5949 | \n",
      "Epoch 00140 | Loss 1.1169 | Train Acc. 0.5693 | \n",
      "Epoch 00145 | Loss 1.1776 | Train Acc. 0.5323 | \n",
      "Epoch 00150 | Loss 1.1370 | Train Acc. 0.5603 | \n",
      "Epoch 00155 | Loss 1.1991 | Train Acc. 0.5324 | \n",
      "Epoch 00160 | Loss 1.1715 | Train Acc. 0.5672 | \n",
      "Epoch 00165 | Loss 1.0451 | Train Acc. 0.5772 | \n",
      "Epoch 00170 | Loss 1.2100 | Train Acc. 0.5336 | \n",
      "Epoch 00175 | Loss 1.1379 | Train Acc. 0.5422 | \n",
      "Epoch 00180 | Loss 1.1117 | Train Acc. 0.5706 | \n",
      "Epoch 00185 | Loss 1.1002 | Train Acc. 0.5842 | \n",
      "Epoch 00190 | Loss 1.1740 | Train Acc. 0.5564 | \n",
      "Epoch 00195 | Loss 1.0955 | Train Acc. 0.5638 | \n",
      "Epoch 00200 | Loss 1.1458 | Train Acc. 0.5546 | \n",
      "Epoch 00205 | Loss 1.1139 | Train Acc. 0.5633 | \n",
      "Epoch 00210 | Loss 1.1549 | Train Acc. 0.5540 | \n",
      "Epoch 00215 | Loss 1.0932 | Train Acc. 0.5425 | \n",
      "Epoch 00220 | Loss 1.1492 | Train Acc. 0.5476 | \n",
      "Epoch 00225 | Loss 1.1718 | Train Acc. 0.5332 | \n",
      "Epoch 00230 | Loss 1.1298 | Train Acc. 0.5656 | \n",
      "Epoch 00235 | Loss 1.1594 | Train Acc. 0.5421 | \n",
      "Epoch 00240 | Loss 1.1399 | Train Acc. 0.5651 | \n",
      "Epoch 00245 | Loss 1.0523 | Train Acc. 0.5848 | \n",
      "Epoch 00250 | Loss 1.0813 | Train Acc. 0.5908 | \n",
      "Epoch 00255 | Loss 1.0409 | Train Acc. 0.5899 | \n",
      "Epoch 00260 | Loss 1.0908 | Train Acc. 0.5534 | \n",
      "Epoch 00265 | Loss 1.1121 | Train Acc. 0.5521 | \n",
      "Epoch 00270 | Loss 1.1731 | Train Acc. 0.5507 | \n",
      "Epoch 00275 | Loss 1.1644 | Train Acc. 0.5334 | \n",
      "Epoch 00280 | Loss 1.0554 | Train Acc. 0.6168 | \n",
      "Epoch 00285 | Loss 1.1728 | Train Acc. 0.5510 | \n",
      "Epoch 00290 | Loss 1.0788 | Train Acc. 0.5774 | \n",
      "Epoch 00295 | Loss 1.0938 | Train Acc. 0.5689 | \n",
      "Epoch 00300 | Loss 1.0929 | Train Acc. 0.5760 | \n",
      "Epoch 00305 | Loss 1.1098 | Train Acc. 0.5612 | \n",
      "Epoch 00310 | Loss 1.1184 | Train Acc. 0.5508 | \n",
      "Epoch 00315 | Loss 1.1069 | Train Acc. 0.5888 | \n",
      "Epoch 00320 | Loss 1.0895 | Train Acc. 0.5721 | \n",
      "Epoch 00325 | Loss 1.1292 | Train Acc. 0.5509 | \n",
      "Epoch 00330 | Loss 1.1450 | Train Acc. 0.5605 | \n",
      "Epoch 00335 | Loss 1.1028 | Train Acc. 0.5790 | \n",
      "Epoch 00340 | Loss 1.1134 | Train Acc. 0.5605 | \n",
      "Epoch 00345 | Loss 1.0938 | Train Acc. 0.5683 | \n",
      "Epoch 00350 | Loss 1.1165 | Train Acc. 0.5602 | \n",
      "Epoch 00355 | Loss 1.1395 | Train Acc. 0.5763 | \n",
      "Early stopping! No improvement for 100 consecutive epochs.\n",
      "<dgl.dataloading.dataloader.DataLoader object at 0x000002576A1A3460>\n",
      "Fold : 3 | Test Accuracy = 0.5323 | F1 = 0.4701 \n",
      "Clearing gpu memory\n",
      "GCN_MME(\n",
      "  (encoder_dims): ModuleList(\n",
      "    (0): Encoder(\n",
      "      (encoder): ModuleList(\n",
      "        (0): Linear(in_features=343, out_features=500, bias=True)\n",
      "        (1): Linear(in_features=500, out_features=16, bias=True)\n",
      "      )\n",
      "      (norm): ModuleList(\n",
      "        (0): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (decoder): Sequential(\n",
      "        (0): Linear(in_features=16, out_features=32, bias=True)\n",
      "      )\n",
      "      (drop): Dropout(p=0.5, inplace=False)\n",
      "    )\n",
      "    (1): Encoder(\n",
      "      (encoder): ModuleList(\n",
      "        (0): Linear(in_features=86, out_features=500, bias=True)\n",
      "        (1): Linear(in_features=500, out_features=32, bias=True)\n",
      "      )\n",
      "      (norm): ModuleList(\n",
      "        (0): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (decoder): Sequential(\n",
      "        (0): Linear(in_features=32, out_features=32, bias=True)\n",
      "      )\n",
      "      (drop): Dropout(p=0.5, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (gnnlayers): ModuleList(\n",
      "    (0): GraphConv(in=32, out=16, normalization=both, activation=None)\n",
      "    (1): GraphConv(in=16, out=4, normalization=both, activation=None)\n",
      "  )\n",
      "  (batch_norms): ModuleList(\n",
      "    (0): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "Graph(num_nodes=1551, num_edges=14980,\n",
      "      ndata_schemes={'feat': Scheme(shape=(429,), dtype=torch.float32), 'label': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={})\n",
      "Epoch 00000 | Loss 1.8485 | Train Acc. 0.3168 | \n",
      "Epoch 00005 | Loss 1.3385 | Train Acc. 0.4742 | \n",
      "Epoch 00010 | Loss 1.2343 | Train Acc. 0.5171 | \n",
      "Epoch 00015 | Loss 1.2958 | Train Acc. 0.5107 | \n",
      "Epoch 00020 | Loss 1.3714 | Train Acc. 0.5055 | \n",
      "Epoch 00025 | Loss 1.2131 | Train Acc. 0.5439 | \n",
      "Epoch 00030 | Loss 1.2008 | Train Acc. 0.5249 | \n",
      "Epoch 00035 | Loss 1.1664 | Train Acc. 0.5365 | \n",
      "Epoch 00040 | Loss 1.2133 | Train Acc. 0.5558 | \n",
      "Epoch 00045 | Loss 1.1033 | Train Acc. 0.5829 | \n",
      "Epoch 00050 | Loss 1.1352 | Train Acc. 0.5610 | \n",
      "Epoch 00055 | Loss 1.1710 | Train Acc. 0.5761 | \n",
      "Epoch 00060 | Loss 1.2434 | Train Acc. 0.5453 | \n",
      "Epoch 00065 | Loss 1.1578 | Train Acc. 0.5547 | \n",
      "Epoch 00070 | Loss 1.0486 | Train Acc. 0.5915 | \n",
      "Epoch 00075 | Loss 1.1143 | Train Acc. 0.5678 | \n",
      "Epoch 00080 | Loss 1.1430 | Train Acc. 0.5871 | \n",
      "Epoch 00085 | Loss 1.2311 | Train Acc. 0.5239 | \n",
      "Epoch 00090 | Loss 1.1781 | Train Acc. 0.5668 | \n",
      "Epoch 00095 | Loss 1.2166 | Train Acc. 0.5521 | \n",
      "Epoch 00100 | Loss 1.1627 | Train Acc. 0.5363 | \n",
      "Epoch 00105 | Loss 1.1211 | Train Acc. 0.5570 | \n",
      "Epoch 00110 | Loss 1.1845 | Train Acc. 0.5191 | \n",
      "Epoch 00115 | Loss 1.1741 | Train Acc. 0.5576 | \n",
      "Epoch 00120 | Loss 1.1370 | Train Acc. 0.5547 | \n",
      "Epoch 00125 | Loss 1.0718 | Train Acc. 0.5955 | \n",
      "Epoch 00130 | Loss 1.1137 | Train Acc. 0.5677 | \n",
      "Epoch 00135 | Loss 1.1288 | Train Acc. 0.5675 | \n",
      "Epoch 00140 | Loss 1.0646 | Train Acc. 0.6100 | \n",
      "Epoch 00145 | Loss 1.1620 | Train Acc. 0.5723 | \n",
      "Epoch 00150 | Loss 1.1717 | Train Acc. 0.5775 | \n",
      "Epoch 00155 | Loss 1.1253 | Train Acc. 0.5623 | \n",
      "Epoch 00160 | Loss 1.1187 | Train Acc. 0.5669 | \n",
      "Epoch 00165 | Loss 1.1552 | Train Acc. 0.5642 | \n",
      "Epoch 00170 | Loss 1.1655 | Train Acc. 0.5649 | \n",
      "Early stopping! No improvement for 100 consecutive epochs.\n",
      "<dgl.dataloading.dataloader.DataLoader object at 0x000002576A1CF820>\n",
      "Fold : 4 | Test Accuracy = 0.5871 | F1 = 0.5722 \n",
      "Clearing gpu memory\n",
      "GCN_MME(\n",
      "  (encoder_dims): ModuleList(\n",
      "    (0): Encoder(\n",
      "      (encoder): ModuleList(\n",
      "        (0): Linear(in_features=343, out_features=500, bias=True)\n",
      "        (1): Linear(in_features=500, out_features=16, bias=True)\n",
      "      )\n",
      "      (norm): ModuleList(\n",
      "        (0): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (decoder): Sequential(\n",
      "        (0): Linear(in_features=16, out_features=32, bias=True)\n",
      "      )\n",
      "      (drop): Dropout(p=0.5, inplace=False)\n",
      "    )\n",
      "    (1): Encoder(\n",
      "      (encoder): ModuleList(\n",
      "        (0): Linear(in_features=86, out_features=500, bias=True)\n",
      "        (1): Linear(in_features=500, out_features=32, bias=True)\n",
      "      )\n",
      "      (norm): ModuleList(\n",
      "        (0): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (decoder): Sequential(\n",
      "        (0): Linear(in_features=32, out_features=32, bias=True)\n",
      "      )\n",
      "      (drop): Dropout(p=0.5, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (gnnlayers): ModuleList(\n",
      "    (0): GraphConv(in=32, out=16, normalization=both, activation=None)\n",
      "    (1): GraphConv(in=16, out=4, normalization=both, activation=None)\n",
      "  )\n",
      "  (batch_norms): ModuleList(\n",
      "    (0): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "Graph(num_nodes=1551, num_edges=14980,\n",
      "      ndata_schemes={'feat': Scheme(shape=(429,), dtype=torch.float32), 'label': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={})\n",
      "Epoch 00000 | Loss 1.5973 | Train Acc. 0.3712 | \n",
      "Epoch 00005 | Loss 1.3065 | Train Acc. 0.4877 | \n",
      "Epoch 00010 | Loss 1.2899 | Train Acc. 0.4833 | \n",
      "Epoch 00015 | Loss 1.2490 | Train Acc. 0.5212 | \n",
      "Epoch 00020 | Loss 1.1995 | Train Acc. 0.5421 | \n",
      "Epoch 00025 | Loss 1.1987 | Train Acc. 0.5442 | \n",
      "Epoch 00030 | Loss 1.1535 | Train Acc. 0.5465 | \n",
      "Epoch 00035 | Loss 1.1328 | Train Acc. 0.5566 | \n",
      "Epoch 00040 | Loss 1.1640 | Train Acc. 0.5291 | \n",
      "Epoch 00045 | Loss 1.1188 | Train Acc. 0.5568 | \n",
      "Epoch 00050 | Loss 1.1479 | Train Acc. 0.5600 | \n",
      "Epoch 00055 | Loss 1.1590 | Train Acc. 0.5647 | \n",
      "Epoch 00060 | Loss 1.1276 | Train Acc. 0.5476 | \n",
      "Epoch 00065 | Loss 1.1845 | Train Acc. 0.5202 | \n",
      "Epoch 00070 | Loss 1.1810 | Train Acc. 0.5272 | \n",
      "Epoch 00075 | Loss 1.1600 | Train Acc. 0.5205 | \n",
      "Epoch 00080 | Loss 1.1288 | Train Acc. 0.5535 | \n",
      "Epoch 00085 | Loss 1.0695 | Train Acc. 0.5730 | \n",
      "Epoch 00090 | Loss 1.0964 | Train Acc. 0.5770 | \n",
      "Epoch 00095 | Loss 1.1757 | Train Acc. 0.5478 | \n",
      "Epoch 00100 | Loss 1.1114 | Train Acc. 0.5689 | \n",
      "Epoch 00105 | Loss 1.1373 | Train Acc. 0.5601 | \n",
      "Epoch 00110 | Loss 1.0819 | Train Acc. 0.5539 | \n",
      "Epoch 00115 | Loss 1.1608 | Train Acc. 0.5654 | \n",
      "Epoch 00120 | Loss 1.1957 | Train Acc. 0.5404 | \n",
      "Epoch 00125 | Loss 1.1564 | Train Acc. 0.5411 | \n",
      "Epoch 00130 | Loss 1.0895 | Train Acc. 0.5649 | \n",
      "Epoch 00135 | Loss 1.1598 | Train Acc. 0.5519 | \n",
      "Epoch 00140 | Loss 1.1583 | Train Acc. 0.5532 | \n",
      "Epoch 00145 | Loss 1.1151 | Train Acc. 0.5781 | \n",
      "Epoch 00150 | Loss 1.1412 | Train Acc. 0.5610 | \n",
      "Epoch 00155 | Loss 1.0263 | Train Acc. 0.5857 | \n",
      "Epoch 00160 | Loss 1.0729 | Train Acc. 0.5723 | \n",
      "Epoch 00165 | Loss 1.0641 | Train Acc. 0.5993 | \n",
      "Epoch 00170 | Loss 1.1465 | Train Acc. 0.5499 | \n",
      "Epoch 00175 | Loss 1.1361 | Train Acc. 0.5456 | \n",
      "Epoch 00180 | Loss 1.1008 | Train Acc. 0.5668 | \n",
      "Epoch 00185 | Loss 1.1197 | Train Acc. 0.5555 | \n",
      "Epoch 00190 | Loss 1.1738 | Train Acc. 0.5289 | \n",
      "Epoch 00195 | Loss 1.1902 | Train Acc. 0.5524 | \n",
      "Epoch 00200 | Loss 1.2240 | Train Acc. 0.5462 | \n",
      "Epoch 00205 | Loss 1.1364 | Train Acc. 0.5632 | \n",
      "Epoch 00210 | Loss 1.1525 | Train Acc. 0.5665 | \n",
      "Epoch 00215 | Loss 1.0436 | Train Acc. 0.5797 | \n",
      "Epoch 00220 | Loss 1.1243 | Train Acc. 0.5728 | \n",
      "Epoch 00225 | Loss 1.1224 | Train Acc. 0.5571 | \n",
      "Epoch 00230 | Loss 1.1778 | Train Acc. 0.5485 | \n",
      "Epoch 00235 | Loss 1.1174 | Train Acc. 0.5848 | \n",
      "Epoch 00240 | Loss 1.1219 | Train Acc. 0.5750 | \n",
      "Epoch 00245 | Loss 1.1024 | Train Acc. 0.5713 | \n",
      "Epoch 00250 | Loss 1.1471 | Train Acc. 0.5732 | \n",
      "Epoch 00255 | Loss 1.0560 | Train Acc. 0.6130 | \n",
      "Early stopping! No improvement for 100 consecutive epochs.\n",
      "<dgl.dataloading.dataloader.DataLoader object at 0x000002576A1CEDA0>\n",
      "Fold : 5 | Test Accuracy = 0.6258 | F1 = 0.5881 \n",
      "Clearing gpu memory\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_logits = []\n",
    "test_labels = []\n",
    "output_metrics = []\n",
    "test_indices = [] \n",
    "for i, (train_index, test_index) in enumerate(skf.split(meta.index, meta['diagnosis'])):\n",
    "    test_indices.append(test_index)\n",
    "    model_name = 'GCN_MME'\n",
    "    # Initialize model\n",
    "    if pnet : \n",
    "        model = model_mapping[model](MME_input_shapes , latent_dim , decoder_dim , h_feats,  len(meta.unique()), PNet=net).to(device)\n",
    "    else :\n",
    "        #model = model_mapping[model](MME_input_shapes , latent_dim , decoder_dim , h_feats,  len(meta.unique())).to(device)\n",
    "        #model = model_mapping[model](MME_input_shapes , latent_dim , decoder_dim , h_feats,  len(meta['diagnosis'].unique())).to(device)#改\n",
    "        model=GCN_MME(MME_input_shapes , [16 , 32] , 32 , [16]  , len(meta['diagnosis'].unique())).to(device)\n",
    "    print(model)\n",
    "    print(g)\n",
    "    \n",
    "    g = g.to(device)\n",
    "\n",
    "    # Train the model\n",
    "    loss_plot = train(g, train_index, device ,  model , label , epochs , lr , patience)\n",
    "    plt.title(f'Loss for split {i}')\n",
    "    save_path = output + '/loss_plots/'\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    plt.savefig(f'{save_path}loss_split_{i}.png' , dpi = 200)\n",
    "    plt.clf()\n",
    "    \n",
    "    sampler = NeighborSampler(\n",
    "        [15 for i in range(len(model.gnnlayers))],  # fanout for each layer\n",
    "        prefetch_node_feats=['feat'],\n",
    "        prefetch_labels=['label'],\n",
    "    )\n",
    "    test_dataloader = DataLoader(\n",
    "        g,\n",
    "        torch.Tensor(test_index).to(torch.int64).to(device),\n",
    "        sampler,\n",
    "        device=device,\n",
    "        batch_size=1024,\n",
    "        shuffle=True,\n",
    "        drop_last=False,\n",
    "        num_workers=0,\n",
    "        use_uva=False,\n",
    "    )\n",
    "    print(test_dataloader)\n",
    "    # Evaluate the model\n",
    "    test_output_metrics = evaluate(model , g , test_dataloader)\n",
    "\n",
    "    print(\n",
    "        \"Fold : {:01d} | Test Accuracy = {:.4f} | F1 = {:.4f} \".format(\n",
    "        i+1 , test_output_metrics[1] , test_output_metrics[2] )\n",
    "    )\n",
    "    \n",
    "    # 这里假设 test_output_metrics[-2] 和 test_output_metrics[-1] 是张量\n",
    "    test_logits.extend(test_output_metrics[-2].detach().cpu().numpy().tolist())  # 将 Tensor 转换为列表\n",
    "    test_labels.extend(test_output_metrics[-1].detach().cpu().numpy().tolist())   # 将 Tens\n",
    "    \n",
    "    if interpret_feat : \n",
    "        prev_dim = 0\n",
    "        for i_int , (pnet , dim) in enumerate(zip(model.encoder_dims , model.input_dims)) : \n",
    "\n",
    "            pnet.features = features[i_int]\n",
    "\n",
    "            x = g.ndata['feat'][torch.Tensor(test_index).to(device).to(torch.int) , prev_dim:dim+prev_dim]\n",
    "\n",
    "            if i_int == 0 :\n",
    "                model_importances_cv = interpret(pnet , x , savedir='None' , plot=False)\n",
    "                for layer in model_importances_cv.keys() : \n",
    "                    model_importances_cv[layer] = model_importances_cv[layer].fillna(0)\n",
    "                model_importances_cv['Features'] = (model_importances_cv['Features'] - model_importances_cv['Features'].mean().mean())/model_importances_cv['Features'].mean().std()\n",
    "                model_importances_cv['Features'] = model_importances_cv['Features'].abs().mean(axis=0)\n",
    "            else : \n",
    "                model_importances_tmp = interpret(pnet , x , savedir='None', plot=False)\n",
    "                model_importances_tmp['Features'] = (model_importances_tmp['Features'] - model_importances_tmp['Features'].mean().mean())/model_importances_tmp['Features'].mean().std()\n",
    "                model_importances_tmp['Features'] = model_importances_tmp['Features'].abs().mean(axis=0)\n",
    "                for layer in model_importances_cv.keys() : \n",
    "                    model_importances_tmp[layer] = model_importances_tmp[layer].fillna(0)\n",
    "                    if layer == 'Features' : \n",
    "                        model_importances_cv[layer] = pd.concat([model_importances_cv[layer] , model_importances_tmp[layer]])\n",
    "                    else : \n",
    "                        model_importances_cv[layer] += model_importances_tmp[layer]\n",
    "\n",
    "            prev_dim += dim\n",
    "\n",
    "        model_importances_cv = {k: (v.divide(i_int+1) if k != 'Features' else v) for k, v in model_importances_cv.items()}\n",
    "        if i == 0 : \n",
    "            model_importances = model_importances_cv\n",
    "        else : \n",
    "            for layer in model_importances.keys() :\n",
    "                if layer == 'Features' : \n",
    "                    model_importances[layer] +=  model_importances_cv[layer]\n",
    "                else : \n",
    "                    model_importances[layer] = pd.concat([model_importances[layer] , model_importances_cv[layer]] , axis=0).reset_index(drop=True)\n",
    "    \n",
    "    # Save the output metrics and best performing model\n",
    "    output_metrics.append(test_output_metrics)\n",
    "    if i == 0 : \n",
    "        best_model = model\n",
    "        best_idx = i\n",
    "    elif output_metrics[best_idx][1] < test_output_metrics[1] : \n",
    "        best_model = model\n",
    "        best_idx   = i\n",
    "    # 保存最佳模型的测试集结果\n",
    "    if output_metrics:  # 确保 output_metrics 不为空\n",
    "        best_test_metrics = output_metrics[best_idx]\n",
    "        best_test_accuracy = best_test_metrics[1]\n",
    "        best_test_f1 = best_test_metrics[2]\n",
    "        best_test_logits = test_logits  # 或者从 output_metrics 中提取\n",
    "        best_test_labels = test_labels  # 或者从 output_metrics 中提取\n",
    "\n",
    "        # 将结果保存到文件\n",
    "        import pandas as pd\n",
    "\n",
    "        results_df = pd.DataFrame({\n",
    "            'Test Accuracy': [best_test_accuracy],\n",
    "            'Test F1': [best_test_f1],\n",
    "            'Test Logits': [best_test_logits],\n",
    "            'Test Labels': [best_test_labels],\n",
    "            'Best Model Index': [test_index]  # 添加最佳模型索引\n",
    "        })\n",
    "    #get_gpu_memory()\n",
    "    del model\n",
    "    gc.collect()\n",
    "    #torch.cuda.empty_cache()\n",
    "    print('Clearing gpu memory')\n",
    "    #get_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Test Accuracy  Test F1                                        Test Logits  \\\n",
      "0       0.625806  0.54068  [[-0.030222222208976746, -0.166040301322937, 0...   \n",
      "\n",
      "                                         Test Labels  \\\n",
      "0  [2, 1, 2, 3, 2, 2, 2, 3, 3, 3, 2, 0, 2, 3, 3, ...   \n",
      "\n",
      "                                    Best Model Index  \n",
      "0  [3, 6, 15, 16, 18, 24, 25, 33, 43, 47, 48, 58,...  \n"
     ]
    }
   ],
   "source": [
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_expanded = pd.DataFrame(results_df['Test Logits'].tolist(), index=results_df.index)\n",
    "labels_expanded = pd.DataFrame(results_df['Test Labels'].tolist(), index=results_df.index)\n",
    "best_model_index_expanded = pd.DataFrame(results_df['Best Model Index'].tolist(), index=results_df.index)\n",
    "\n",
    "# 将所有数据合并到一个新的 DataFrame 中\n",
    "final_df = pd.concat(\n",
    "    [results_df[['Test Accuracy', 'Test F1']], logits_expanded, labels_expanded, best_model_index_expanded],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0    1    2    3    4    5    6    7    8    9    ...   300   301   302  \\\n",
      "0    3    6   15   16   18   24   25   33   43   47  ...  1499  1512  1513   \n",
      "\n",
      "    303   304   305   306   307   308   309  \n",
      "0  1518  1524  1536  1541  1543  1546  1548  \n",
      "\n",
      "[1 rows x 310 columns]\n",
      "   0     1     2     3     4     5     6     7     8     9     ...  1541  \\\n",
      "0     2     1     2     3     2     2     2     3     3     3  ...     2   \n",
      "\n",
      "   1542  1543  1544  1545  1546  1547  1548  1549  1550  \n",
      "0     0     2     3     3     2     2     2     3     3  \n",
      "\n",
      "[1 rows x 1551 columns]\n"
     ]
    }
   ],
   "source": [
    "print(best_model_index_expanded )\n",
    "print(labels_expanded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最佳模型的测试结果已保存到 'best_model_test_results.csv'。\n"
     ]
    }
   ],
   "source": [
    "# 保存 DataFrame 到 CSV 文件\n",
    "best_model_index_expanded.to_csv('D:/a.csv', index=False)\n",
    "labels_expanded.to_csv('D:/b.csv', index=False)\n",
    "print(f\"最佳模型的测试结果已保存到 'best_model_test_results.csv'。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_logits = torch.stack(test_logits)\n",
    "#test_labels = torch.stack(test_labels)\n",
    "\n",
    "if interpret_feat : \n",
    "    model_importances = {k: (v.divide(i+1)) for k, v in model_importances.items()}\n",
    "    with open(f'{output}/model_importance.pkl', 'wb') as file:\n",
    "        pickle.dump(model_importances, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Fold Cross Validation Accuracy = 59.32 ± 3.43\n",
      "5 Fold Cross Validation F1 = 53.65 ± 4.24\n"
     ]
    }
   ],
   "source": [
    "# Save the output metrics to a file   \n",
    "accuracy = []\n",
    "F1 = []\n",
    "output_file = output + '/' + \"test_metrics.txt\"\n",
    "with open(output_file , 'w') as f :\n",
    "    i = 0\n",
    "    for metric in output_metrics :\n",
    "        i += 1\n",
    "        f.write(\"Fold %i \\n\" % i)\n",
    "        f.write(f\"acc = %2.3f , avg_prc = %2.3f , avg_recall = %2.3f , avg_f1 = %2.3f\" % \n",
    "                (metric[1] , metric[3] , metric[4] , metric[2]))\n",
    "        f.write('\\n')\n",
    "        accuracy.append(metric[1])\n",
    "        F1.append(metric[2])\n",
    "        \n",
    "    f.write('-------------------------\\n')\n",
    "    f.write(\"%i Fold Cross Validation Accuracy = %2.2f \\u00B1 %2.2f \\n\" %(n_splits , np.mean(accuracy)*100 , np.std(accuracy)*100))\n",
    "    f.write(\"%i Fold Cross Validation F1 = %2.2f \\u00B1 %2.2f \\n\" %(n_splits , np.mean(F1)*100 , np.std(F1)*100))\n",
    "    f.write('-------------------------\\n')\n",
    "\n",
    "print(\"%i Fold Cross Validation Accuracy = %2.2f \\u00B1 %2.2f\" %(5 , np.mean(accuracy)*100 , np.std(accuracy)*100))\n",
    "print(\"%i Fold Cross Validation F1 = %2.2f \\u00B1 %2.2f\" %(5 , np.mean(F1)*100 , np.std(F1)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current date\n",
    "current_date = datetime.now()\n",
    "\n",
    "# Extract month and day as string names\n",
    "month = current_date.strftime('%B')[:3]  # Full month name\n",
    "day = current_date.day\n",
    "\n",
    "save_path = output + '/Models/'\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "torch.save({\n",
    "    'model_state_dict': best_model.state_dict(),\n",
    "    # You can add more information to save, such as training history, hyperparameters, etc.\n",
    "}, f'{save_path}GCN_MME_model_{month}{day}' )\n",
    "torch.save(best_model,\"./训练/result/Models/best_model.model\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 16.754208469390868 minutes\n"
     ]
    }
   ],
   "source": [
    "# Stop the timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate and print the elapsed time\n",
    "elapsed_time = (end_time - start_time)/60\n",
    "print(f\"Elapsed time: {elapsed_time} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modality: path, Shape: (832, 343)\n",
      "Modality: cell, Shape: (832, 86)\n"
     ]
    }
   ],
   "source": [
    "input=\"外部测试\"\n",
    "modalities=['path', 'cell']\n",
    "target='diagnosis'\n",
    "index_col='sample_id'\n",
    "label_file=\"labels.csv\"\n",
    "# Load data and metadata\n",
    "datModalities , meta = data_parsing(input , modalities , target , index_col,label_file)\n",
    "for mod, expr in datModalities.items():\n",
    "    print(f\"Modality: {mod}, Shape: {expr.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = meta.loc[sorted(meta.index)]\n",
    "# 将 diagnosis 列转换为分类数据类型\n",
    "diagnosis_series = meta['diagnosis'].astype('category')\n",
    "\n",
    "# 获取独热编码\n",
    "label = F.one_hot(torch.Tensor(diagnosis_series.cat.codes).to(torch.int64))\n",
    "MME_input_shapes = [ datModalities[mod].shape[1] for mod in datModalities]\n",
    "h = reduce(merge_dfs , list(datModalities.values()))\n",
    "h = h.loc[sorted(h.index)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 832\n",
      "Number of edges: 7216\n",
      "Graph(num_nodes=832, num_edges=7216,\n",
      "      ndata_schemes={'feat': Scheme(shape=(429,), dtype=torch.float32), 'label': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={})\n"
     ]
    }
   ],
   "source": [
    "# 读取 CSV 文件\n",
    "csv_file = '外部测试\\\\snf_graph.csv'\n",
    "df = pd.read_csv(csv_file, na_values=['NA'])\n",
    "\n",
    "# 创建一个有向图\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# 添加边到图中\n",
    "for index, row in df.iterrows():\n",
    "    G.add_edge(row['from'], row['to'], \n",
    "               from_color=row['from_frame.color'], \n",
    "               from_name=row['from_name'], \n",
    "               from_class=row['from_class'], \n",
    "               from_vertex_color=row['from_vertex.frame.color'],\n",
    "               to_color=row['to_frame.color'], \n",
    "               to_name=row['to_name'], \n",
    "               to_class=row['to_class'], \n",
    "               to_vertex_color=row['to_vertex.frame.color'])\n",
    "\n",
    "# 将 NetworkX 图转换为 DGL 图\n",
    "g = dgl.from_networkx(G)\n",
    "\n",
    "# 打印图的一些基本信息\n",
    "print(\"Number of nodes:\", g.number_of_nodes())\n",
    "print(\"Number of edges:\", g.number_of_edges())\n",
    "\n",
    "# 这里可以为节点和边添加特征，如果需要的话\n",
    "# 例如，可以为节点添加一个特征张量\n",
    "g.ndata['feat'] = torch.Tensor(h.to_numpy())\n",
    "#node_features = torch.zeros(g.number_of_nodes(), 3)  # 假设每个节点有3个特征\n",
    "#g.ndata['feat'] = node_features\n",
    "\n",
    "# 如果需要为边添加特征\n",
    "#edge_features = torch.zeros(g.number_of_edges(), 4)  # 假设每条边有4个特征\n",
    "#g.edata['feat'] = edge_features\n",
    "g.ndata['label'] = torch.tensor(meta['diagnosis'], dtype=torch.int64)\n",
    "# 输出图的结构\n",
    "print(g)\n",
    "g = dgl.add_self_loop(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GCN_MME(\n",
       "  (encoder_dims): ModuleList(\n",
       "    (0): Encoder(\n",
       "      (encoder): ModuleList(\n",
       "        (0): Linear(in_features=343, out_features=500, bias=True)\n",
       "        (1): Linear(in_features=500, out_features=16, bias=True)\n",
       "      )\n",
       "      (norm): ModuleList(\n",
       "        (0): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=16, out_features=32, bias=True)\n",
       "      )\n",
       "      (drop): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (1): Encoder(\n",
       "      (encoder): ModuleList(\n",
       "        (0): Linear(in_features=86, out_features=500, bias=True)\n",
       "        (1): Linear(in_features=500, out_features=32, bias=True)\n",
       "      )\n",
       "      (norm): ModuleList(\n",
       "        (0): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (decoder): Sequential(\n",
       "        (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "      )\n",
       "      (drop): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (gnnlayers): ModuleList(\n",
       "    (0): GraphConv(in=32, out=16, normalization=both, activation=None)\n",
       "    (1): GraphConv(in=16, out=4, normalization=both, activation=None)\n",
       "  )\n",
       "  (batch_norms): ModuleList(\n",
       "    (0): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (drop): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_path = '训练\\\\result\\\\Models\\\\GCN_MME_model_一月6'  # 替换为你的模型路径\n",
    "# 加载最佳模型\n",
    "best_model = GCN_MME(MME_input_shapes , [16 , 32] , 32 , [16]  , len(meta['diagnosis'].unique())).to(device)\n",
    "checkpoint = torch.load(best_model_path )\n",
    "best_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "best_model.eval()  # 设置模型为评估模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<dgl.dataloading.dataloader.DataLoader object at 0x0000025752E17E80>\n",
      "Fold : 6 | Test Accuracy = 0.4507 | F1 = 0.4712 \n"
     ]
    }
   ],
   "source": [
    "test_logits = []\n",
    "test_labels = []\n",
    "test_dataloader = DataLoader(\n",
    "    g,\n",
    "    torch.arange(g.num_nodes()).to(device),\n",
    "    sampler,\n",
    "    device=device,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    "    num_workers=0,\n",
    "    use_uva=False,\n",
    ")\n",
    "print(test_dataloader)\n",
    "# Evaluate the model\n",
    "test_output_metrics = evaluate(best_model , g , test_dataloader)\n",
    "\n",
    "print(\n",
    "    \"Fold : {:01d} | Test Accuracy = {:.4f} | F1 = {:.4f} \".format(\n",
    "    i+1 , test_output_metrics[1] , test_output_metrics[2] )\n",
    ")\n",
    "\n",
    "# 这里假设 test_output_metrics[-2] 和 test_output_metrics[-1] 是张量\n",
    "test_logits.extend(test_output_metrics[-2].detach().cpu().numpy().tolist())  # 将 Tensor 转换为列表\n",
    "test_labels.extend(test_output_metrics[-1].detach().cpu().numpy().tolist())   # 将 Tens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 2, 3, 0, 2, 2, 0, 2, 0, 2, 2, 3, 3, 3, 2, 2, 0, 0, 0, 2, 2, 2, 3, 2, 3, 2, 2, 0, 3, 2, 3, 2, 2, 2, 0, 2, 2, 2, 3, 3, 0, 3, 2, 3, 0, 1, 1, 3, 2, 2, 2, 2, 3, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 2, 3, 0, 0, 2, 2, 3, 2, 2, 2, 2, 1, 0, 2, 2, 2, 0, 2, 2, 1, 2, 2, 2, 3, 2, 2, 0, 2, 2, 0, 2, 0, 1, 2, 2, 0, 0, 2, 2, 3, 1, 2, 2, 3, 1, 2, 0, 2, 2, 2, 2, 2, 3, 3, 0, 2, 2, 2, 1, 3, 3, 0, 2, 0, 2, 3, 2, 2, 2, 2, 2, 0, 3, 2, 0, 2, 3, 2, 0, 2, 2, 0, 2, 2, 2, 2, 2, 2, 0, 3, 0, 2, 3, 3, 2, 0, 3, 0, 2, 3, 3, 0, 0, 3, 2, 2, 0, 2, 2, 2, 1, 2, 0, 3, 2, 0, 3, 2, 2, 2, 2, 3, 2, 2, 0, 1, 1, 2, 2, 2, 2, 2, 0, 1, 3, 2, 0, 2, 2, 3, 2, 0, 0, 0, 2, 3, 3, 3, 2, 1, 0, 0, 2, 2, 1, 0, 2, 2, 2, 2, 1, 2, 0, 2, 2, 3, 2, 2, 0, 1, 2, 3, 1, 0, 3, 2, 0, 0, 3, 2, 0, 2, 2, 2, 1, 1, 2, 3, 0, 2, 2, 3, 2, 2, 3, 3, 3, 3, 2, 2, 2, 2, 3, 2, 2, 1, 2, 1, 3, 2, 2, 2, 1, 3, 3, 0, 3, 1, 3, 2, 1, 0, 2, 3, 2, 2, 2, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 3, 2, 2, 2, 2, 2, 2, 3, 3, 0, 0, 2, 3, 3, 3, 0, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 3, 3, 2, 3, 2, 2, 2, 1, 1, 2, 3, 0, 2, 2, 2, 3, 2, 2, 2, 3, 3, 2, 3, 2, 2, 3, 2, 3, 0, 3, 2, 2, 2, 1, 3, 2, 1, 2, 0, 3, 2, 3, 1, 2, 0, 2, 2, 2, 0, 3, 2, 1, 2, 2, 2, 1, 2, 3, 2, 3, 2, 2, 2, 3, 2, 1, 2, 3, 3, 3, 2, 3, 3, 0, 2, 2, 2, 1, 2, 1, 2, 3, 0, 2, 3, 2, 3, 2, 2, 3, 3, 1, 2, 2, 2, 2, 2, 3, 0, 2, 2, 2, 3, 2, 0, 2, 1, 2, 0, 3, 3, 2, 2, 0, 2, 3, 2, 2, 2, 2, 3, 3, 1, 2, 2, 2, 0, 2, 2, 2, 3, 3, 2, 2, 3, 2, 2, 2, 3, 2, 2, 3, 1, 2, 2, 2, 2, 0, 0, 2, 2, 3, 2, 2, 2, 2, 2, 1, 0, 1, 2, 0, 0, 3, 2, 2, 0, 3, 1, 3, 3, 2, 2, 2, 2, 0, 3, 2, 2, 3, 2, 2, 2, 2, 3, 2, 0, 0, 2, 2, 1, 2, 2, 3, 0, 2, 0, 2, 3, 1, 0, 0, 3, 1, 2, 2, 2, 0, 3, 2, 3, 2, 2, 2, 1, 2, 1, 0, 0, 2, 2, 2, 2, 3, 2, 0, 2, 1, 3, 3, 3, 2, 3, 2, 2, 2, 2, 2, 3, 2, 0, 3, 0, 2, 0, 0, 2, 2, 2, 0, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 1, 2, 2, 3, 3, 2, 2, 2, 3, 2, 0, 3, 1, 2, 1, 2, 2, 3, 3, 2, 3, 0, 2, 3, 3, 0, 2, 0, 1, 2, 3, 0, 3, 3, 2, 3, 3, 2, 3, 2, 0, 0, 1, 2, 1, 2, 3, 2, 3, 1, 2, 2, 2, 1, 1, 3, 2, 2, 2, 3, 2, 2, 3, 1, 3, 2, 2, 0, 0, 0, 3, 3, 0, 2, 2, 2, 0, 2, 0, 1, 2, 2, 2, 0, 3, 2, 2, 3, 2, 1, 2, 2, 0, 1, 3, 2, 3, 0, 2, 2, 2, 3, 3, 2, 2, 0, 0, 3, 2, 0, 3, 0, 2, 0, 2, 2, 3, 2, 0, 2, 2, 3, 3, 2, 3, 2, 2, 3, 0, 3, 2, 2, 3, 0, 3, 2, 2, 3, 0, 2, 2, 2, 3, 3, 2, 2, 0, 0, 2, 3, 0, 2, 1, 2, 2, 2, 0, 2, 0, 0, 0, 2, 0, 2, 2, 1, 3, 0, 3, 2, 2, 2, 2, 3, 2, 2, 0, 0, 2, 2, 2, 2, 3, 0, 2, 3, 0, 3, 2, 2, 0, 3, 2, 2, 0, 3, 3, 1, 2, 2, 2, 3, 3, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 3, 2, 0, 2, 3, 0, 2, 2, 2, 2, 3, 3, 3, 2, 2, 3, 1, 2, 2, 2, 2, 0, 0, 2, 0, 1, 2, 0, 2]\n"
     ]
    }
   ],
   "source": [
    "print(test_labels)\n",
    "df = pd.DataFrame(test_labels, columns=['Predicted_Label'])\n",
    "\n",
    "# 保存为 CSV 文件\n",
    "df.to_csv('D:/predict.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
