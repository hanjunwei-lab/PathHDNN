{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Library Import \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from datetime import datetime\n",
    "\n",
    "from MAIN.utils import *\n",
    "from MAIN.train import *\n",
    "import MAIN.preprocess_functions\n",
    "from MAIN.GNN_MME import GCN_MME , GSage_MME , GAT_MME\n",
    "\n",
    "from Modules.PNetTorch.MAIN.reactome import ReactomeNetwork\n",
    "from Modules.PNetTorch.MAIN.Pnet import MaskedLinear , PNET\n",
    "from Modules.PNetTorch.MAIN.utils import numpy_array_to_one_hot, get_gpu_memory\n",
    "from Modules.PNetTorch.MAIN.interpret import interpret , evaluate_interpret_save , visualize_importances\n",
    "\n",
    "import dgl\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"Finished Library Import \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# Define the merge operation setup\n",
    "def merge_dfs(left_df, right_df):\n",
    "    # Merging on 'key' and expanding with 'how=outer' to include all records\n",
    "    return pd.merge(left_df, right_df, left_index=True, right_index=True, how='outer')\n",
    "no_cuda=True\n",
    "output=\"data(li)delete10\\\\result\\\\\"\n",
    "# Map model names to class objects\n",
    "model_mapping = {\n",
    "    \"GCN\": GCN_MME,\n",
    "    \"GSage\": GSage_MME,\n",
    "    'GAT': GAT_MME\n",
    "}\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Check if output directory exists, if not create it\n",
    "if not os.path.exists(output) : \n",
    "    os.makedirs(output, exist_ok=True)\n",
    "    \n",
    "# Specify the device to use\n",
    "device = torch.device('cpu' if no_cuda else 'cuda') # Get GPU device name, else use CPU\n",
    "print(\"Using %s device\" % device)\n",
    "#get_gpu_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modality: exp, Shape: (36, 20789)\n",
      "Modality: cna, Shape: (36, 16286)\n",
      "Modality: mut, Shape: (36, 10086)\n",
      "Number of nodes: 36\n",
      "Number of edges: 294\n",
      "Graph(num_nodes=36, num_edges=294,\n",
      "      ndata_schemes={'feat': Scheme(shape=(47161,), dtype=torch.float32), 'label': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={})\n",
      "StratifiedKFold(n_splits=3, random_state=None, shuffle=True)\n"
     ]
    }
   ],
   "source": [
    "input=\"data(li)delete10\"\n",
    "modalities=['exp', 'cna','mut']\n",
    "target='diagnosis'\n",
    "index_col='sample_id'\n",
    "label_file=\"labels.csv\"\n",
    "# Load data and metadata\n",
    "datModalities , meta = data_parsing(input , modalities , target , index_col,label_file)\n",
    "for mod, expr in datModalities.items():\n",
    "    print(f\"Modality: {mod}, Shape: {expr.shape}\")\n",
    "interpret_feat=False\n",
    "pnet=False\n",
    "if interpret_feat : \n",
    "    features = {}\n",
    "    for i , mod in enumerate(datModalities) : \n",
    "        features[i] = list(datModalities[mod].columns)\n",
    "\n",
    "if pnet : \n",
    "    # List of genes of interest in PNet (keep to less than 1000 for big models)\n",
    "    genes = pd.read_csv(f'{input}/../ext_data/genelist.txt', header=0)\n",
    "\n",
    "    # Build network to obtain gene and pathway relationships\n",
    "    net = ReactomeNetwork(genes_of_interest=np.unique(list(genes['genes'].values)) , n_levels=5)\n",
    "meta = meta.loc[sorted(meta.index)]\n",
    "# 将 diagnosis 列转换为分类数据类型\n",
    "diagnosis_series = meta['diagnosis'].astype('category')\n",
    "\n",
    "# 获取独热编码\n",
    "label = F.one_hot(torch.Tensor(diagnosis_series.cat.codes).to(torch.int64))\n",
    "\n",
    "\n",
    "MME_input_shapes = [ datModalities[mod].shape[1] for mod in datModalities]\n",
    "h = reduce(merge_dfs , list(datModalities.values()))\n",
    "h = h.loc[sorted(h.index)]\n",
    "# 读取 CSV 文件\n",
    "csv_file = 'data(li)delete10\\\\snf_graph.csv'\n",
    "df = pd.read_csv(csv_file, na_values=['NA'])\n",
    "\n",
    "# 创建一个有向图\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# 添加边到图中\n",
    "for index, row in df.iterrows():\n",
    "    G.add_edge(row['from'], row['to'], \n",
    "               from_color=row['from_frame.color'], \n",
    "               from_name=row['from_name'], \n",
    "               from_class=row['from_class'], \n",
    "               from_vertex_color=row['from_vertex.frame.color'],\n",
    "               to_color=row['to_frame.color'], \n",
    "               to_name=row['to_name'], \n",
    "               to_class=row['to_class'], \n",
    "               to_vertex_color=row['to_vertex.frame.color'])\n",
    "\n",
    "# 将 NetworkX 图转换为 DGL 图\n",
    "g = dgl.from_networkx(G)\n",
    "\n",
    "# 打印图的一些基本信息\n",
    "print(\"Number of nodes:\", g.number_of_nodes())\n",
    "print(\"Number of edges:\", g.number_of_edges())\n",
    "\n",
    "# 这里可以为节点和边添加特征，如果需要的话\n",
    "# 例如，可以为节点添加一个特征张量\n",
    "g.ndata['feat'] = torch.Tensor(h.to_numpy())\n",
    "#node_features = torch.zeros(g.number_of_nodes(), 3)  # 假设每个节点有3个特征\n",
    "#g.ndata['feat'] = node_features\n",
    "\n",
    "# 如果需要为边添加特征\n",
    "#edge_features = torch.zeros(g.number_of_edges(), 4)  # 假设每条边有4个特征\n",
    "#g.edata['feat'] = edge_features\n",
    "g.ndata['label'] = torch.tensor(meta['diagnosis'], dtype=torch.int64)\n",
    "# 输出图的结构\n",
    "print(g)\n",
    "g = dgl.add_self_loop(g)\n",
    "no_shuffle=False\n",
    "n_splits=3\n",
    "# Generate K Fold splits\n",
    "if no_shuffle : \n",
    "    skf = StratifiedKFold(n_splits=n_splits , shuffle=False) \n",
    "else :\n",
    "    skf = StratifiedKFold(n_splits=n_splits , shuffle=True) \n",
    "\n",
    "print(skf)\n",
    "\n",
    "output_metrics = []\n",
    "test_logits = []\n",
    "test_labels = []\n",
    "latent_dim=[32,16]\n",
    "model=\"GCN\"\n",
    "decoder_dim=64\n",
    "h_feats= [64]\n",
    "epochs=1000\n",
    "lr=0.001\n",
    "patience=100\n",
    "output=\"data(li)delete10\\\\result\\\\\"\n",
    "interpret_feat=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN_MME(\n",
      "  (encoder_dims): ModuleList(\n",
      "    (0): Encoder(\n",
      "      (encoder): ModuleList(\n",
      "        (0): Linear(in_features=20789, out_features=500, bias=True)\n",
      "        (1): Linear(in_features=500, out_features=16, bias=True)\n",
      "      )\n",
      "      (norm): ModuleList(\n",
      "        (0): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (decoder): Sequential(\n",
      "        (0): Linear(in_features=16, out_features=32, bias=True)\n",
      "      )\n",
      "      (drop): Dropout(p=0.5, inplace=False)\n",
      "    )\n",
      "    (1): Encoder(\n",
      "      (encoder): ModuleList(\n",
      "        (0): Linear(in_features=16286, out_features=500, bias=True)\n",
      "        (1): Linear(in_features=500, out_features=32, bias=True)\n",
      "      )\n",
      "      (norm): ModuleList(\n",
      "        (0): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (decoder): Sequential(\n",
      "        (0): Linear(in_features=32, out_features=32, bias=True)\n",
      "      )\n",
      "      (drop): Dropout(p=0.5, inplace=False)\n",
      "    )\n",
      "    (2): Encoder(\n",
      "      (encoder): ModuleList(\n",
      "        (0): Linear(in_features=10086, out_features=500, bias=True)\n",
      "        (1): Linear(in_features=500, out_features=32, bias=True)\n",
      "      )\n",
      "      (norm): ModuleList(\n",
      "        (0): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (decoder): Sequential(\n",
      "        (0): Linear(in_features=32, out_features=32, bias=True)\n",
      "      )\n",
      "      (drop): Dropout(p=0.5, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (gnnlayers): ModuleList(\n",
      "    (0): GraphConv(in=32, out=16, normalization=both, activation=None)\n",
      "    (1): GraphConv(in=16, out=2, normalization=both, activation=None)\n",
      "  )\n",
      "  (batch_norms): ModuleList(\n",
      "    (0): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "Graph(num_nodes=36, num_edges=330,\n",
      "      ndata_schemes={'feat': Scheme(shape=(47161,), dtype=torch.float32), 'label': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={})\n",
      "Epoch 00000 | Loss 1.0023 | Train Acc. 0.2500 | \n",
      "Epoch 00005 | Loss 0.5163 | Train Acc. 0.8333 | \n",
      "Epoch 00010 | Loss 0.4694 | Train Acc. 0.8333 | \n",
      "Epoch 00015 | Loss 0.4603 | Train Acc. 0.7917 | \n",
      "Epoch 00020 | Loss 0.7099 | Train Acc. 0.6250 | \n",
      "Epoch 00025 | Loss 0.3184 | Train Acc. 0.9167 | \n",
      "Epoch 00030 | Loss 0.3830 | Train Acc. 0.8750 | \n",
      "Epoch 00035 | Loss 0.4183 | Train Acc. 0.9167 | \n",
      "Epoch 00040 | Loss 0.4066 | Train Acc. 0.7917 | \n",
      "Epoch 00045 | Loss 0.4335 | Train Acc. 0.7917 | \n",
      "Epoch 00050 | Loss 0.3352 | Train Acc. 0.9167 | \n",
      "Epoch 00055 | Loss 0.4215 | Train Acc. 0.7917 | \n",
      "Epoch 00060 | Loss 0.5574 | Train Acc. 0.7083 | \n",
      "Epoch 00065 | Loss 0.2887 | Train Acc. 0.9167 | \n",
      "Epoch 00070 | Loss 0.3521 | Train Acc. 0.8750 | \n",
      "Epoch 00075 | Loss 0.4522 | Train Acc. 0.8750 | \n",
      "Epoch 00080 | Loss 0.3636 | Train Acc. 0.8750 | \n",
      "Epoch 00085 | Loss 0.4088 | Train Acc. 0.8333 | \n",
      "Epoch 00090 | Loss 0.2963 | Train Acc. 0.8750 | \n",
      "Epoch 00095 | Loss 0.3122 | Train Acc. 0.9583 | \n",
      "Epoch 00100 | Loss 0.3586 | Train Acc. 0.8333 | \n",
      "Epoch 00105 | Loss 0.2350 | Train Acc. 0.8750 | \n",
      "Epoch 00110 | Loss 0.3579 | Train Acc. 0.8750 | \n",
      "Epoch 00115 | Loss 0.2799 | Train Acc. 0.9167 | \n",
      "Epoch 00120 | Loss 0.2842 | Train Acc. 0.8750 | \n",
      "Epoch 00125 | Loss 0.2982 | Train Acc. 0.8333 | \n",
      "Epoch 00130 | Loss 0.5549 | Train Acc. 0.8333 | \n",
      "Epoch 00135 | Loss 0.2899 | Train Acc. 0.9167 | \n",
      "Epoch 00140 | Loss 0.4294 | Train Acc. 0.8333 | \n",
      "Epoch 00145 | Loss 0.2875 | Train Acc. 0.8750 | \n",
      "Epoch 00150 | Loss 0.2982 | Train Acc. 0.8333 | \n",
      "Epoch 00155 | Loss 0.3141 | Train Acc. 0.9583 | \n",
      "Epoch 00160 | Loss 0.4054 | Train Acc. 0.8750 | \n",
      "Epoch 00165 | Loss 0.2976 | Train Acc. 0.8333 | \n",
      "Epoch 00170 | Loss 0.3167 | Train Acc. 0.9167 | \n",
      "Epoch 00175 | Loss 0.2685 | Train Acc. 0.9167 | \n",
      "Epoch 00180 | Loss 0.2526 | Train Acc. 0.9167 | \n",
      "Epoch 00185 | Loss 0.3533 | Train Acc. 0.8750 | \n",
      "Epoch 00190 | Loss 0.2745 | Train Acc. 0.9167 | \n",
      "Epoch 00195 | Loss 0.3248 | Train Acc. 0.8750 | \n",
      "Epoch 00200 | Loss 0.2279 | Train Acc. 0.8750 | \n",
      "Epoch 00205 | Loss 0.2923 | Train Acc. 0.9167 | \n",
      "Epoch 00210 | Loss 0.2678 | Train Acc. 0.9167 | \n",
      "Epoch 00215 | Loss 0.2478 | Train Acc. 0.9583 | \n",
      "Epoch 00220 | Loss 0.2854 | Train Acc. 0.9167 | \n",
      "Epoch 00225 | Loss 0.2756 | Train Acc. 0.9583 | \n",
      "Epoch 00230 | Loss 0.2887 | Train Acc. 0.9167 | \n",
      "Epoch 00235 | Loss 0.2948 | Train Acc. 0.8750 | \n",
      "Epoch 00240 | Loss 0.2711 | Train Acc. 0.9167 | \n",
      "Epoch 00245 | Loss 0.4141 | Train Acc. 0.7500 | \n",
      "Epoch 00250 | Loss 0.4086 | Train Acc. 0.8333 | \n",
      "Epoch 00255 | Loss 0.3833 | Train Acc. 0.8750 | \n",
      "Epoch 00260 | Loss 0.4126 | Train Acc. 0.7917 | \n",
      "Epoch 00265 | Loss 0.2897 | Train Acc. 0.8750 | \n",
      "Epoch 00270 | Loss 0.3678 | Train Acc. 0.8750 | \n",
      "Epoch 00275 | Loss 0.2138 | Train Acc. 0.9167 | \n",
      "Epoch 00280 | Loss 0.3008 | Train Acc. 0.9167 | \n",
      "Epoch 00285 | Loss 0.2133 | Train Acc. 0.9583 | \n",
      "Epoch 00290 | Loss 0.2541 | Train Acc. 0.9167 | \n",
      "Epoch 00295 | Loss 0.3998 | Train Acc. 0.7917 | \n",
      "Epoch 00300 | Loss 0.2705 | Train Acc. 0.9167 | \n",
      "Epoch 00305 | Loss 0.2709 | Train Acc. 0.9167 | \n",
      "Epoch 00310 | Loss 0.2987 | Train Acc. 0.8750 | \n",
      "Epoch 00315 | Loss 0.2774 | Train Acc. 0.9167 | \n",
      "Epoch 00320 | Loss 0.2562 | Train Acc. 0.9583 | \n",
      "Epoch 00325 | Loss 0.3023 | Train Acc. 0.9583 | \n",
      "Epoch 00330 | Loss 0.4064 | Train Acc. 0.7917 | \n",
      "Epoch 00335 | Loss 0.2363 | Train Acc. 0.8333 | \n",
      "Epoch 00340 | Loss 0.3471 | Train Acc. 0.8750 | \n",
      "Epoch 00345 | Loss 0.2968 | Train Acc. 0.9583 | \n",
      "Epoch 00350 | Loss 0.3766 | Train Acc. 0.8333 | \n",
      "Epoch 00355 | Loss 0.2152 | Train Acc. 0.8750 | \n",
      "Epoch 00360 | Loss 0.3729 | Train Acc. 0.8750 | \n",
      "Epoch 00365 | Loss 0.2224 | Train Acc. 0.9167 | \n",
      "Epoch 00370 | Loss 0.4146 | Train Acc. 0.8333 | \n",
      "Epoch 00375 | Loss 0.2391 | Train Acc. 0.9167 | \n",
      "Epoch 00380 | Loss 0.3049 | Train Acc. 0.8750 | \n",
      "Epoch 00385 | Loss 0.2272 | Train Acc. 0.9167 | \n",
      "Epoch 00390 | Loss 0.2235 | Train Acc. 0.9167 | \n",
      "Epoch 00395 | Loss 0.2268 | Train Acc. 0.9167 | \n",
      "Epoch 00400 | Loss 0.2676 | Train Acc. 0.9167 | \n",
      "Epoch 00405 | Loss 0.2590 | Train Acc. 0.9583 | \n",
      "Epoch 00410 | Loss 0.3809 | Train Acc. 0.8750 | \n",
      "Epoch 00415 | Loss 0.3390 | Train Acc. 0.8750 | \n",
      "Epoch 00420 | Loss 0.2639 | Train Acc. 0.9167 | \n",
      "Epoch 00425 | Loss 0.3470 | Train Acc. 0.8333 | \n",
      "Epoch 00430 | Loss 0.2390 | Train Acc. 0.9583 | \n",
      "Epoch 00435 | Loss 0.2567 | Train Acc. 0.9583 | \n",
      "Epoch 00440 | Loss 0.2356 | Train Acc. 0.9583 | \n",
      "Epoch 00445 | Loss 0.2049 | Train Acc. 0.9583 | \n",
      "Epoch 00450 | Loss 0.3960 | Train Acc. 0.8333 | \n",
      "Epoch 00455 | Loss 0.3035 | Train Acc. 0.9167 | \n",
      "Epoch 00460 | Loss 0.2234 | Train Acc. 0.9167 | \n",
      "Epoch 00465 | Loss 0.3527 | Train Acc. 0.7917 | \n",
      "Epoch 00470 | Loss 0.2676 | Train Acc. 0.8750 | \n",
      "Epoch 00475 | Loss 0.3165 | Train Acc. 0.8750 | \n",
      "Epoch 00480 | Loss 0.2904 | Train Acc. 0.8750 | \n",
      "Epoch 00485 | Loss 0.4035 | Train Acc. 0.8750 | \n",
      "Epoch 00490 | Loss 0.3250 | Train Acc. 0.8750 | \n",
      "Epoch 00495 | Loss 0.2499 | Train Acc. 0.9167 | \n",
      "Epoch 00500 | Loss 0.3307 | Train Acc. 0.8333 | \n",
      "Epoch 00505 | Loss 0.4370 | Train Acc. 0.7917 | \n",
      "Epoch 00510 | Loss 0.2828 | Train Acc. 0.9167 | \n",
      "Epoch 00515 | Loss 0.3377 | Train Acc. 0.8333 | \n",
      "Early stopping! No improvement for 100 consecutive epochs.\n",
      "<dgl.dataloading.dataloader.DataLoader object at 0x000001A8C9FF7D90>\n",
      "Fold : 1 | Test Accuracy = 0.7500 | F1 = 0.2577 \n",
      "Clearing gpu memory\n",
      "GCN_MME(\n",
      "  (encoder_dims): ModuleList(\n",
      "    (0): Encoder(\n",
      "      (encoder): ModuleList(\n",
      "        (0): Linear(in_features=20789, out_features=500, bias=True)\n",
      "        (1): Linear(in_features=500, out_features=16, bias=True)\n",
      "      )\n",
      "      (norm): ModuleList(\n",
      "        (0): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (decoder): Sequential(\n",
      "        (0): Linear(in_features=16, out_features=32, bias=True)\n",
      "      )\n",
      "      (drop): Dropout(p=0.5, inplace=False)\n",
      "    )\n",
      "    (1): Encoder(\n",
      "      (encoder): ModuleList(\n",
      "        (0): Linear(in_features=16286, out_features=500, bias=True)\n",
      "        (1): Linear(in_features=500, out_features=32, bias=True)\n",
      "      )\n",
      "      (norm): ModuleList(\n",
      "        (0): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (decoder): Sequential(\n",
      "        (0): Linear(in_features=32, out_features=32, bias=True)\n",
      "      )\n",
      "      (drop): Dropout(p=0.5, inplace=False)\n",
      "    )\n",
      "    (2): Encoder(\n",
      "      (encoder): ModuleList(\n",
      "        (0): Linear(in_features=10086, out_features=500, bias=True)\n",
      "        (1): Linear(in_features=500, out_features=32, bias=True)\n",
      "      )\n",
      "      (norm): ModuleList(\n",
      "        (0): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (decoder): Sequential(\n",
      "        (0): Linear(in_features=32, out_features=32, bias=True)\n",
      "      )\n",
      "      (drop): Dropout(p=0.5, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (gnnlayers): ModuleList(\n",
      "    (0): GraphConv(in=32, out=16, normalization=both, activation=None)\n",
      "    (1): GraphConv(in=16, out=2, normalization=both, activation=None)\n",
      "  )\n",
      "  (batch_norms): ModuleList(\n",
      "    (0): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "Graph(num_nodes=36, num_edges=330,\n",
      "      ndata_schemes={'feat': Scheme(shape=(47161,), dtype=torch.float32), 'label': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={})\n",
      "Epoch 00000 | Loss 0.7264 | Train Acc. 0.5417 | \n",
      "Epoch 00005 | Loss 0.5181 | Train Acc. 0.7917 | \n",
      "Epoch 00010 | Loss 0.5424 | Train Acc. 0.7917 | \n",
      "Epoch 00015 | Loss 0.5422 | Train Acc. 0.7500 | \n",
      "Epoch 00020 | Loss 0.4457 | Train Acc. 0.7083 | \n",
      "Epoch 00025 | Loss 0.5437 | Train Acc. 0.8333 | \n",
      "Epoch 00030 | Loss 0.4345 | Train Acc. 0.8333 | \n",
      "Epoch 00035 | Loss 0.4324 | Train Acc. 0.8333 | \n",
      "Epoch 00040 | Loss 0.5085 | Train Acc. 0.8333 | \n",
      "Epoch 00045 | Loss 0.4696 | Train Acc. 0.7917 | \n",
      "Epoch 00050 | Loss 0.4653 | Train Acc. 0.8333 | \n",
      "Epoch 00055 | Loss 0.5030 | Train Acc. 0.7083 | \n",
      "Epoch 00060 | Loss 0.4346 | Train Acc. 0.8333 | \n",
      "Epoch 00065 | Loss 0.3675 | Train Acc. 0.8750 | \n",
      "Epoch 00070 | Loss 0.3639 | Train Acc. 0.8333 | \n",
      "Epoch 00075 | Loss 0.3454 | Train Acc. 0.9167 | \n",
      "Epoch 00080 | Loss 0.3940 | Train Acc. 0.8750 | \n",
      "Epoch 00085 | Loss 0.3615 | Train Acc. 0.8333 | \n",
      "Epoch 00090 | Loss 0.4686 | Train Acc. 0.7917 | \n",
      "Epoch 00095 | Loss 0.3854 | Train Acc. 0.8750 | \n",
      "Epoch 00100 | Loss 0.3607 | Train Acc. 0.9167 | \n",
      "Epoch 00105 | Loss 0.3289 | Train Acc. 0.9167 | \n",
      "Epoch 00110 | Loss 0.3218 | Train Acc. 0.9167 | \n",
      "Epoch 00115 | Loss 0.3681 | Train Acc. 0.9167 | \n",
      "Epoch 00120 | Loss 0.3869 | Train Acc. 0.8750 | \n",
      "Epoch 00125 | Loss 0.2822 | Train Acc. 0.8750 | \n",
      "Epoch 00130 | Loss 0.2965 | Train Acc. 0.8750 | \n",
      "Epoch 00135 | Loss 0.2956 | Train Acc. 0.8750 | \n",
      "Epoch 00140 | Loss 0.3288 | Train Acc. 0.9167 | \n",
      "Epoch 00145 | Loss 0.3525 | Train Acc. 0.8750 | \n",
      "Epoch 00150 | Loss 0.2783 | Train Acc. 0.8750 | \n",
      "Epoch 00155 | Loss 0.3433 | Train Acc. 0.8750 | \n",
      "Epoch 00160 | Loss 0.3369 | Train Acc. 0.9167 | \n",
      "Epoch 00165 | Loss 0.3512 | Train Acc. 0.8333 | \n",
      "Epoch 00170 | Loss 0.2753 | Train Acc. 0.9167 | \n",
      "Epoch 00175 | Loss 0.2844 | Train Acc. 0.9167 | \n",
      "Epoch 00180 | Loss 0.3017 | Train Acc. 0.8750 | \n",
      "Epoch 00185 | Loss 0.3271 | Train Acc. 0.8750 | \n",
      "Epoch 00190 | Loss 0.2923 | Train Acc. 0.9167 | \n",
      "Epoch 00195 | Loss 0.2493 | Train Acc. 0.9167 | \n",
      "Epoch 00200 | Loss 0.2924 | Train Acc. 0.8750 | \n",
      "Epoch 00205 | Loss 0.2653 | Train Acc. 0.9167 | \n",
      "Epoch 00210 | Loss 0.2254 | Train Acc. 0.9167 | \n",
      "Epoch 00215 | Loss 0.3439 | Train Acc. 0.9167 | \n",
      "Epoch 00220 | Loss 0.3596 | Train Acc. 0.8750 | \n",
      "Epoch 00225 | Loss 0.3566 | Train Acc. 0.8750 | \n",
      "Epoch 00230 | Loss 0.2985 | Train Acc. 0.8750 | \n",
      "Epoch 00235 | Loss 0.3299 | Train Acc. 0.9167 | \n",
      "Epoch 00240 | Loss 0.2427 | Train Acc. 0.9167 | \n",
      "Epoch 00245 | Loss 0.2446 | Train Acc. 0.9167 | \n",
      "Epoch 00250 | Loss 0.2846 | Train Acc. 0.8750 | \n",
      "Epoch 00255 | Loss 0.2537 | Train Acc. 0.8750 | \n",
      "Epoch 00260 | Loss 0.3012 | Train Acc. 0.8750 | \n",
      "Epoch 00265 | Loss 0.3071 | Train Acc. 0.9167 | \n",
      "Epoch 00270 | Loss 0.3852 | Train Acc. 0.8750 | \n",
      "Epoch 00275 | Loss 0.3598 | Train Acc. 0.8333 | \n",
      "Epoch 00280 | Loss 0.3196 | Train Acc. 0.9167 | \n",
      "Epoch 00285 | Loss 0.3266 | Train Acc. 0.9167 | \n",
      "Epoch 00290 | Loss 0.2616 | Train Acc. 0.8750 | \n",
      "Epoch 00295 | Loss 0.3459 | Train Acc. 0.9167 | \n",
      "Epoch 00300 | Loss 0.3651 | Train Acc. 0.9167 | \n",
      "Epoch 00305 | Loss 0.2562 | Train Acc. 0.9167 | \n",
      "Epoch 00310 | Loss 0.3671 | Train Acc. 0.8750 | \n",
      "Early stopping! No improvement for 100 consecutive epochs.\n",
      "<dgl.dataloading.dataloader.DataLoader object at 0x000001A8CA3BE3E0>\n",
      "Fold : 2 | Test Accuracy = 0.6667 | F1 = 0.2393 \n",
      "Clearing gpu memory\n",
      "GCN_MME(\n",
      "  (encoder_dims): ModuleList(\n",
      "    (0): Encoder(\n",
      "      (encoder): ModuleList(\n",
      "        (0): Linear(in_features=20789, out_features=500, bias=True)\n",
      "        (1): Linear(in_features=500, out_features=16, bias=True)\n",
      "      )\n",
      "      (norm): ModuleList(\n",
      "        (0): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (decoder): Sequential(\n",
      "        (0): Linear(in_features=16, out_features=32, bias=True)\n",
      "      )\n",
      "      (drop): Dropout(p=0.5, inplace=False)\n",
      "    )\n",
      "    (1): Encoder(\n",
      "      (encoder): ModuleList(\n",
      "        (0): Linear(in_features=16286, out_features=500, bias=True)\n",
      "        (1): Linear(in_features=500, out_features=32, bias=True)\n",
      "      )\n",
      "      (norm): ModuleList(\n",
      "        (0): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (decoder): Sequential(\n",
      "        (0): Linear(in_features=32, out_features=32, bias=True)\n",
      "      )\n",
      "      (drop): Dropout(p=0.5, inplace=False)\n",
      "    )\n",
      "    (2): Encoder(\n",
      "      (encoder): ModuleList(\n",
      "        (0): Linear(in_features=10086, out_features=500, bias=True)\n",
      "        (1): Linear(in_features=500, out_features=32, bias=True)\n",
      "      )\n",
      "      (norm): ModuleList(\n",
      "        (0): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (decoder): Sequential(\n",
      "        (0): Linear(in_features=32, out_features=32, bias=True)\n",
      "      )\n",
      "      (drop): Dropout(p=0.5, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (gnnlayers): ModuleList(\n",
      "    (0): GraphConv(in=32, out=16, normalization=both, activation=None)\n",
      "    (1): GraphConv(in=16, out=2, normalization=both, activation=None)\n",
      "  )\n",
      "  (batch_norms): ModuleList(\n",
      "    (0): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "Graph(num_nodes=36, num_edges=330,\n",
      "      ndata_schemes={'feat': Scheme(shape=(47161,), dtype=torch.float32), 'label': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={})\n",
      "Epoch 00000 | Loss 0.5652 | Train Acc. 0.7500 | \n",
      "Epoch 00005 | Loss 0.5096 | Train Acc. 0.7083 | \n",
      "Epoch 00010 | Loss 0.5693 | Train Acc. 0.7500 | \n",
      "Epoch 00015 | Loss 0.4843 | Train Acc. 0.8750 | \n",
      "Epoch 00020 | Loss 0.6770 | Train Acc. 0.6667 | \n",
      "Epoch 00025 | Loss 0.4820 | Train Acc. 0.8333 | \n",
      "Epoch 00030 | Loss 0.3727 | Train Acc. 0.8750 | \n",
      "Epoch 00035 | Loss 0.5977 | Train Acc. 0.7500 | \n",
      "Epoch 00040 | Loss 0.4965 | Train Acc. 0.8333 | \n",
      "Epoch 00045 | Loss 0.4192 | Train Acc. 0.8750 | \n",
      "Epoch 00050 | Loss 0.3441 | Train Acc. 0.8750 | \n",
      "Epoch 00055 | Loss 0.4693 | Train Acc. 0.7083 | \n",
      "Epoch 00060 | Loss 0.4610 | Train Acc. 0.8750 | \n",
      "Epoch 00065 | Loss 0.3851 | Train Acc. 0.9167 | \n",
      "Epoch 00070 | Loss 0.3039 | Train Acc. 0.8333 | \n",
      "Epoch 00075 | Loss 0.3955 | Train Acc. 0.7917 | \n",
      "Epoch 00080 | Loss 0.3588 | Train Acc. 0.9167 | \n",
      "Epoch 00085 | Loss 0.3541 | Train Acc. 0.7917 | \n",
      "Epoch 00090 | Loss 0.3681 | Train Acc. 0.8333 | \n",
      "Epoch 00095 | Loss 0.3481 | Train Acc. 0.8333 | \n",
      "Epoch 00100 | Loss 0.3727 | Train Acc. 0.9167 | \n",
      "Epoch 00105 | Loss 0.3556 | Train Acc. 0.8333 | \n",
      "Epoch 00110 | Loss 0.2182 | Train Acc. 0.9583 | \n",
      "Epoch 00115 | Loss 0.2756 | Train Acc. 0.9167 | \n",
      "Epoch 00120 | Loss 0.3561 | Train Acc. 0.8750 | \n",
      "Epoch 00125 | Loss 0.3815 | Train Acc. 0.9167 | \n",
      "Epoch 00130 | Loss 0.3148 | Train Acc. 0.8750 | \n",
      "Epoch 00135 | Loss 0.2685 | Train Acc. 0.9167 | \n",
      "Epoch 00140 | Loss 0.2787 | Train Acc. 0.9583 | \n",
      "Epoch 00145 | Loss 0.3601 | Train Acc. 0.8750 | \n",
      "Epoch 00150 | Loss 0.3035 | Train Acc. 0.9167 | \n",
      "Epoch 00155 | Loss 0.3292 | Train Acc. 0.8750 | \n",
      "Epoch 00160 | Loss 0.3734 | Train Acc. 0.8333 | \n",
      "Epoch 00165 | Loss 0.1946 | Train Acc. 0.8750 | \n",
      "Epoch 00170 | Loss 0.2655 | Train Acc. 0.9167 | \n",
      "Epoch 00175 | Loss 0.3172 | Train Acc. 0.9167 | \n",
      "Epoch 00180 | Loss 0.3581 | Train Acc. 0.9167 | \n",
      "Epoch 00185 | Loss 0.2913 | Train Acc. 0.9167 | \n",
      "Epoch 00190 | Loss 0.2861 | Train Acc. 0.9583 | \n",
      "Epoch 00195 | Loss 0.2616 | Train Acc. 0.9167 | \n",
      "Epoch 00200 | Loss 0.3386 | Train Acc. 0.8750 | \n",
      "Epoch 00205 | Loss 0.2473 | Train Acc. 0.9167 | \n",
      "Epoch 00210 | Loss 0.2054 | Train Acc. 0.9167 | \n",
      "Epoch 00215 | Loss 0.3020 | Train Acc. 0.8750 | \n",
      "Epoch 00220 | Loss 0.2659 | Train Acc. 0.9583 | \n",
      "Epoch 00225 | Loss 0.1847 | Train Acc. 0.9167 | \n",
      "Epoch 00230 | Loss 0.2448 | Train Acc. 0.9583 | \n",
      "Epoch 00235 | Loss 0.2844 | Train Acc. 0.9167 | \n",
      "Epoch 00240 | Loss 0.2683 | Train Acc. 0.9167 | \n",
      "Epoch 00245 | Loss 0.2040 | Train Acc. 0.9583 | \n",
      "Epoch 00250 | Loss 0.3023 | Train Acc. 0.9167 | \n",
      "Epoch 00255 | Loss 0.2695 | Train Acc. 0.9167 | \n",
      "Epoch 00260 | Loss 0.3507 | Train Acc. 0.8750 | \n",
      "Epoch 00265 | Loss 0.2364 | Train Acc. 0.9583 | \n",
      "Epoch 00270 | Loss 0.2890 | Train Acc. 0.9167 | \n",
      "Epoch 00275 | Loss 0.2545 | Train Acc. 0.8750 | \n",
      "Epoch 00280 | Loss 0.2483 | Train Acc. 0.9583 | \n",
      "Epoch 00285 | Loss 0.3673 | Train Acc. 0.9167 | \n",
      "Epoch 00290 | Loss 0.3441 | Train Acc. 0.9167 | \n",
      "Epoch 00295 | Loss 0.3060 | Train Acc. 0.9167 | \n",
      "Epoch 00300 | Loss 0.3053 | Train Acc. 0.7917 | \n",
      "Epoch 00305 | Loss 0.2253 | Train Acc. 0.9583 | \n",
      "Epoch 00310 | Loss 0.2456 | Train Acc. 0.9583 | \n",
      "Epoch 00315 | Loss 0.3521 | Train Acc. 0.8750 | \n",
      "Epoch 00320 | Loss 0.2356 | Train Acc. 0.9583 | \n",
      "Epoch 00325 | Loss 0.2584 | Train Acc. 0.9583 | \n",
      "Epoch 00330 | Loss 0.3762 | Train Acc. 0.9167 | \n",
      "Epoch 00335 | Loss 0.2764 | Train Acc. 0.8750 | \n",
      "Epoch 00340 | Loss 0.3766 | Train Acc. 0.8333 | \n",
      "Epoch 00345 | Loss 0.2660 | Train Acc. 0.9167 | \n",
      "Epoch 00350 | Loss 0.2388 | Train Acc. 0.8750 | \n",
      "Epoch 00355 | Loss 0.2177 | Train Acc. 1.0000 | \n",
      "Epoch 00360 | Loss 0.3656 | Train Acc. 0.7917 | \n",
      "Epoch 00365 | Loss 0.2967 | Train Acc. 0.9167 | \n",
      "Epoch 00370 | Loss 0.2126 | Train Acc. 1.0000 | \n",
      "Epoch 00375 | Loss 0.3001 | Train Acc. 0.8750 | \n",
      "Epoch 00380 | Loss 0.2085 | Train Acc. 0.9167 | \n",
      "Epoch 00385 | Loss 0.2745 | Train Acc. 0.9167 | \n",
      "Epoch 00390 | Loss 0.2457 | Train Acc. 0.9167 | \n",
      "Epoch 00395 | Loss 0.2842 | Train Acc. 0.8333 | \n",
      "Epoch 00400 | Loss 0.2365 | Train Acc. 0.9167 | \n",
      "Epoch 00405 | Loss 0.2164 | Train Acc. 1.0000 | \n",
      "Epoch 00410 | Loss 0.3047 | Train Acc. 0.8750 | \n",
      "Epoch 00415 | Loss 0.2275 | Train Acc. 1.0000 | \n",
      "Epoch 00420 | Loss 0.3571 | Train Acc. 0.8333 | \n",
      "Epoch 00425 | Loss 0.2991 | Train Acc. 0.8750 | \n",
      "Early stopping! No improvement for 100 consecutive epochs.\n",
      "<dgl.dataloading.dataloader.DataLoader object at 0x000001A8D1F5E800>\n",
      "Fold : 3 | Test Accuracy = 0.7500 | F1 = 0.2727 \n",
      "Clearing gpu memory\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_logits = []\n",
    "test_labels = []\n",
    "output_metrics = []\n",
    "test_indices = [] \n",
    "for i, (train_index, test_index) in enumerate(skf.split(meta.index, meta['diagnosis'])):\n",
    "    test_indices.append(test_index)\n",
    "    model_name = 'GCN_MME'\n",
    "    # Initialize model\n",
    "    if pnet : \n",
    "        model = model_mapping[model](MME_input_shapes , latent_dim , decoder_dim , h_feats,  len(meta.unique()), PNet=net).to(device)\n",
    "    else :\n",
    "        #model = model_mapping[model](MME_input_shapes , latent_dim , decoder_dim , h_feats,  len(meta.unique())).to(device)\n",
    "        #model = model_mapping[model](MME_input_shapes , latent_dim , decoder_dim , h_feats,  len(meta['diagnosis'].unique())).to(device)#改\n",
    "        model=GCN_MME(MME_input_shapes , [16 , 32,32] , 32 , [16]  , len(meta['diagnosis'].unique())).to(device)\n",
    "    print(model)\n",
    "    print(g)\n",
    "    \n",
    "    g = g.to(device)\n",
    "\n",
    "    # Train the model\n",
    "    loss_plot = train(g, train_index, device ,  model , label , epochs , lr , patience)\n",
    "    plt.title(f'Loss for split {i}')\n",
    "    save_path = output + '/loss_plots/'\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    plt.savefig(f'{save_path}loss_split_{i}.png' , dpi = 200)\n",
    "    plt.clf()\n",
    "    \n",
    "    sampler = NeighborSampler(\n",
    "        [15 for i in range(len(model.gnnlayers))],  # fanout for each layer\n",
    "        prefetch_node_feats=['feat'],\n",
    "        prefetch_labels=['label'],\n",
    "    )\n",
    "    test_dataloader = DataLoader(\n",
    "        g,\n",
    "        torch.Tensor(test_index).to(torch.int64).to(device),\n",
    "        sampler,\n",
    "        device=device,\n",
    "        batch_size=1024,\n",
    "        shuffle=True,\n",
    "        drop_last=False,\n",
    "        num_workers=0,\n",
    "        use_uva=False,\n",
    "    )\n",
    "    print(test_dataloader)\n",
    "    # Evaluate the model\n",
    "    test_output_metrics = evaluate(model , g , test_dataloader)\n",
    "\n",
    "    print(\n",
    "        \"Fold : {:01d} | Test Accuracy = {:.4f} | F1 = {:.4f} \".format(\n",
    "        i+1 , test_output_metrics[1] , test_output_metrics[2] )\n",
    "    )\n",
    "    \n",
    "    # 这里假设 test_output_metrics[-2] 和 test_output_metrics[-1] 是张量\n",
    "    test_logits.extend(test_output_metrics[-2].detach().cpu().numpy().tolist())  # 将 Tensor 转换为列表\n",
    "    test_labels.extend(test_output_metrics[-1].detach().cpu().numpy().tolist())   # 将 Tens\n",
    "    \n",
    "    if interpret_feat : \n",
    "        prev_dim = 0\n",
    "        for i_int , (pnet , dim) in enumerate(zip(model.encoder_dims , model.input_dims)) : \n",
    "\n",
    "            pnet.features = features[i_int]\n",
    "\n",
    "            x = g.ndata['feat'][torch.Tensor(test_index).to(device).to(torch.int) , prev_dim:dim+prev_dim]\n",
    "\n",
    "            if i_int == 0 :\n",
    "                model_importances_cv = interpret(pnet , x , savedir='None' , plot=False)\n",
    "                for layer in model_importances_cv.keys() : \n",
    "                    model_importances_cv[layer] = model_importances_cv[layer].fillna(0)\n",
    "                model_importances_cv['Features'] = (model_importances_cv['Features'] - model_importances_cv['Features'].mean().mean())/model_importances_cv['Features'].mean().std()\n",
    "                model_importances_cv['Features'] = model_importances_cv['Features'].abs().mean(axis=0)\n",
    "            else : \n",
    "                model_importances_tmp = interpret(pnet , x , savedir='None', plot=False)\n",
    "                model_importances_tmp['Features'] = (model_importances_tmp['Features'] - model_importances_tmp['Features'].mean().mean())/model_importances_tmp['Features'].mean().std()\n",
    "                model_importances_tmp['Features'] = model_importances_tmp['Features'].abs().mean(axis=0)\n",
    "                for layer in model_importances_cv.keys() : \n",
    "                    model_importances_tmp[layer] = model_importances_tmp[layer].fillna(0)\n",
    "                    if layer == 'Features' : \n",
    "                        model_importances_cv[layer] = pd.concat([model_importances_cv[layer] , model_importances_tmp[layer]])\n",
    "                    else : \n",
    "                        model_importances_cv[layer] += model_importances_tmp[layer]\n",
    "\n",
    "            prev_dim += dim\n",
    "\n",
    "        model_importances_cv = {k: (v.divide(i_int+1) if k != 'Features' else v) for k, v in model_importances_cv.items()}\n",
    "        if i == 0 : \n",
    "            model_importances = model_importances_cv\n",
    "        else : \n",
    "            for layer in model_importances.keys() :\n",
    "                if layer == 'Features' : \n",
    "                    model_importances[layer] +=  model_importances_cv[layer]\n",
    "                else : \n",
    "                    model_importances[layer] = pd.concat([model_importances[layer] , model_importances_cv[layer]] , axis=0).reset_index(drop=True)\n",
    "    \n",
    "    # Save the output metrics and best performing model\n",
    "    output_metrics.append(test_output_metrics)\n",
    "    if i == 0 : \n",
    "        best_model = model\n",
    "        best_idx = i\n",
    "    elif output_metrics[best_idx][1] < test_output_metrics[1] : \n",
    "        best_model = model\n",
    "        best_idx   = i\n",
    "    # 保存最佳模型的测试集结果\n",
    "    if output_metrics:  # 确保 output_metrics 不为空\n",
    "        best_test_metrics = output_metrics[best_idx]\n",
    "        best_test_accuracy = best_test_metrics[1]\n",
    "        best_test_f1 = best_test_metrics[2]\n",
    "        best_test_logits = test_logits  # 或者从 output_metrics 中提取\n",
    "        best_test_labels = test_labels  # 或者从 output_metrics 中提取\n",
    "\n",
    "        # 将结果保存到文件\n",
    "        import pandas as pd\n",
    "\n",
    "        results_df = pd.DataFrame({\n",
    "            'Test Accuracy': [best_test_accuracy],\n",
    "            'Test F1': [best_test_f1],\n",
    "            'Test Logits': [best_test_logits],\n",
    "            'Test Labels': [best_test_labels],\n",
    "            'Best Model Index': [test_index]  # 添加最佳模型索引\n",
    "        })\n",
    "    #get_gpu_memory()\n",
    "    del model\n",
    "    gc.collect()\n",
    "    #torch.cuda.empty_cache()\n",
    "    print('Clearing gpu memory')\n",
    "    #get_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0   1   2   3   4   5   6   7   8   9   10  11\n",
      "0   4   6   8  10  11  12  14  26  30  31  33  34\n",
      "   0   1   2   3   4   5   6   7   8   9   ...  26  27  28  29  30  31  32  \\\n",
      "0   1   0   0   0   0   0   0   0   1   0  ...   0   1   0   0   0   0   0   \n",
      "\n",
      "   33  34  35  \n",
      "0   1   0   0  \n",
      "\n",
      "[1 rows x 36 columns]\n",
      "最佳模型的测试结果已保存到 'best_model_test_results.csv'。\n",
      "5 Fold Cross Validation Accuracy = 72.22 ± 3.93\n",
      "5 Fold Cross Validation F1 = 25.66 ± 1.37\n"
     ]
    }
   ],
   "source": [
    "logits_expanded = pd.DataFrame(results_df['Test Logits'].tolist(), index=results_df.index)\n",
    "labels_expanded = pd.DataFrame(results_df['Test Labels'].tolist(), index=results_df.index)\n",
    "best_model_index_expanded = pd.DataFrame(results_df['Best Model Index'].tolist(), index=results_df.index)\n",
    "\n",
    "# 将所有数据合并到一个新的 DataFrame 中\n",
    "final_df = pd.concat(\n",
    "    [results_df[['Test Accuracy', 'Test F1']], logits_expanded, labels_expanded, best_model_index_expanded],\n",
    "    axis=1\n",
    ")\n",
    "print(best_model_index_expanded )\n",
    "print(labels_expanded)\n",
    "# 保存 DataFrame 到 CSV 文件\n",
    "best_model_index_expanded.to_csv('D:\\\\比较方法\\\\比较方法\\\\MOGDx-main(5)\\\\data(li)delete10\\\\a.csv', index=False)\n",
    "labels_expanded.to_csv('D:\\\\比较方法\\\\比较方法\\\\MOGDx-main(5)\\\\data(li)delete10\\\\b.csv', index=False)\n",
    "print(f\"最佳模型的测试结果已保存到 'best_model_test_results.csv'。\")\n",
    "#test_logits = torch.stack(test_logits)\n",
    "#test_labels = torch.stack(test_labels)\n",
    "\n",
    "if interpret_feat : \n",
    "    model_importances = {k: (v.divide(i+1)) for k, v in model_importances.items()}\n",
    "    with open(f'{output}/model_importance.pkl', 'wb') as file:\n",
    "        pickle.dump(model_importances, file)\n",
    "# Save the output metrics to a file   \n",
    "accuracy = []\n",
    "F1 = []\n",
    "output_file = output + '/' + \"test_metrics.txt\"\n",
    "with open(output_file , 'w') as f :\n",
    "    i = 0\n",
    "    for metric in output_metrics :\n",
    "        i += 1\n",
    "        f.write(\"Fold %i \\n\" % i)\n",
    "        f.write(f\"acc = %2.3f , avg_prc = %2.3f , avg_recall = %2.3f , avg_f1 = %2.3f\" % \n",
    "                (metric[1] , metric[3] , metric[4] , metric[2]))\n",
    "        f.write('\\n')\n",
    "        accuracy.append(metric[1])\n",
    "        F1.append(metric[2])\n",
    "        \n",
    "    f.write('-------------------------\\n')\n",
    "    f.write(\"%i Fold Cross Validation Accuracy = %2.2f \\u00B1 %2.2f \\n\" %(n_splits , np.mean(accuracy)*100 , np.std(accuracy)*100))\n",
    "    f.write(\"%i Fold Cross Validation F1 = %2.2f \\u00B1 %2.2f \\n\" %(n_splits , np.mean(F1)*100 , np.std(F1)*100))\n",
    "    f.write('-------------------------\\n')\n",
    "\n",
    "print(\"%i Fold Cross Validation Accuracy = %2.2f \\u00B1 %2.2f\" %(5 , np.mean(accuracy)*100 , np.std(accuracy)*100))\n",
    "print(\"%i Fold Cross Validation F1 = %2.2f \\u00B1 %2.2f\" %(5 , np.mean(F1)*100 , np.std(F1)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modality: exp, Shape: (16, 20789)\n",
      "Modality: cna, Shape: (16, 16286)\n",
      "Modality: mut, Shape: (16, 10086)\n",
      "Number of nodes: 16\n",
      "Number of edges: 120\n",
      "Graph(num_nodes=16, num_edges=120,\n",
      "      ndata_schemes={'feat': Scheme(shape=(47161,), dtype=torch.float32), 'label': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={})\n",
      "<dgl.dataloading.dataloader.DataLoader object at 0x000001A8C8B09450>\n",
      "Fold : 4 | Test Accuracy = 0.4375 | F1 = 0.4738 \n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "# Get the current date\n",
    "current_date = datetime.now()\n",
    "\n",
    "# Extract month and day as string names\n",
    "month = current_date.strftime('%B')[:3]  # Full month name\n",
    "day = current_date.day\n",
    "\n",
    "save_path = output + '/Models/'\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "torch.save({\n",
    "    'model_state_dict': best_model.state_dict(),\n",
    "    # You can add more information to save, such as training history, hyperparameters, etc.\n",
    "}, f'{save_path}GCN_MME_model_{month}{day}' )\n",
    "torch.save(best_model,\"./data(li)delete10/result/Models/best_model.model\")\n",
    "input=\"data(li2)\\\\外部测试\"\n",
    "modalities=['exp', 'cna','mut']\n",
    "target='diagnosis'\n",
    "index_col='sample_id'\n",
    "label_file=\"labels.csv\"\n",
    "# Load data and metadata\n",
    "datModalities , meta = data_parsing(input , modalities , target , index_col,label_file)\n",
    "for mod, expr in datModalities.items():\n",
    "    print(f\"Modality: {mod}, Shape: {expr.shape}\")\n",
    "    meta = meta.loc[sorted(meta.index)]\n",
    "# 将 diagnosis 列转换为分类数据类型\n",
    "diagnosis_series = meta['diagnosis'].astype('category')\n",
    "\n",
    "# 获取独热编码\n",
    "label = F.one_hot(torch.Tensor(diagnosis_series.cat.codes).to(torch.int64))\n",
    "MME_input_shapes = [ datModalities[mod].shape[1] for mod in datModalities]\n",
    "h = reduce(merge_dfs , list(datModalities.values()))\n",
    "h = h.loc[sorted(h.index)]\n",
    "# 读取 CSV 文件\n",
    "csv_file = 'data(li2)\\\\外部测试\\\\snf_graph.csv'\n",
    "df = pd.read_csv(csv_file, na_values=['NA'])\n",
    "\n",
    "# 创建一个有向图\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# 添加边到图中\n",
    "for index, row in df.iterrows():\n",
    "    G.add_edge(row['from'], row['to'], \n",
    "               from_color=row['from_frame.color'], \n",
    "               from_name=row['from_name'], \n",
    "               from_class=row['from_class'], \n",
    "               from_vertex_color=row['from_vertex.frame.color'],\n",
    "               to_color=row['to_frame.color'], \n",
    "               to_name=row['to_name'], \n",
    "               to_class=row['to_class'], \n",
    "               to_vertex_color=row['to_vertex.frame.color'])\n",
    "\n",
    "# 将 NetworkX 图转换为 DGL 图\n",
    "g = dgl.from_networkx(G)\n",
    "\n",
    "# 打印图的一些基本信息\n",
    "print(\"Number of nodes:\", g.number_of_nodes())\n",
    "print(\"Number of edges:\", g.number_of_edges())\n",
    "\n",
    "# 这里可以为节点和边添加特征，如果需要的话\n",
    "# 例如，可以为节点添加一个特征张量\n",
    "g.ndata['feat'] = torch.Tensor(h.to_numpy())\n",
    "#node_features = torch.zeros(g.number_of_nodes(), 3)  # 假设每个节点有3个特征\n",
    "#g.ndata['feat'] = node_features\n",
    "\n",
    "# 如果需要为边添加特征\n",
    "#edge_features = torch.zeros(g.number_of_edges(), 4)  # 假设每条边有4个特征\n",
    "#g.edata['feat'] = edge_features\n",
    "g.ndata['label'] = torch.tensor(meta['diagnosis'], dtype=torch.int64)\n",
    "# 输出图的结构\n",
    "print(g)\n",
    "g = dgl.add_self_loop(g)\n",
    "best_model_path = 'data(li)delete10\\\\result\\\\Models\\\\GCN_MME_model_Sep2'  # 替换为你的模型路径\n",
    "# 加载最佳模型\n",
    "best_model = GCN_MME(MME_input_shapes , [16 , 32,32] , 32 , [16]  , len(meta['diagnosis'].unique())).to(device)\n",
    "checkpoint = torch.load(best_model_path )\n",
    "best_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "best_model.eval()  # 设置模型为评估模式\n",
    "test_logits = []\n",
    "test_labels = []\n",
    "test_dataloader = DataLoader(\n",
    "    g,\n",
    "    torch.arange(g.num_nodes()).to(device),\n",
    "    sampler,\n",
    "    device=device,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    "    num_workers=0,\n",
    "    use_uva=False,\n",
    ")\n",
    "print(test_dataloader)\n",
    "# Evaluate the model\n",
    "test_output_metrics = evaluate(best_model , g , test_dataloader)\n",
    "\n",
    "print(\n",
    "    \"Fold : {:01d} | Test Accuracy = {:.4f} | F1 = {:.4f} \".format(\n",
    "    i+1 , test_output_metrics[1] , test_output_metrics[2] )\n",
    ")\n",
    "\n",
    "# 这里假设 test_output_metrics[-2] 和 test_output_metrics[-1] 是张量\n",
    "test_logits.extend(test_output_metrics[-2].detach().cpu().numpy().tolist())  # 将 Tensor 转换为列表\n",
    "test_labels.extend(test_output_metrics[-1].detach().cpu().numpy().tolist())   # 将 Tens\n",
    "print(test_labels)\n",
    "df = pd.DataFrame(test_labels, columns=['Predicted_Label'])\n",
    "\n",
    "# 保存为 CSV 文件\n",
    "df.to_csv('D:\\\\比较方法\\\\比较方法\\\\MOGDx-main(5)\\\\data(li)delete10\\\\predict.csv', index=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 4.640977084636688 minutes\n"
     ]
    }
   ],
   "source": [
    "# Stop the timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate and print the elapsed time\n",
    "elapsed_time = (end_time - start_time)/60\n",
    "print(f\"Elapsed time: {elapsed_time} minutes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
