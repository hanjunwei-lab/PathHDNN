{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "###先进行之前的emb_path的训练\n",
    "#Options\n",
    "addRawFeat = True\n",
    "base_path = ''\n",
    "feature_networks_integration = [ 'path','cell']\n",
    "#feature_networks_integration = [ 'exp']\n",
    "node_networks = [ 'path','cell']\n",
    "#node_networks = [ 'exp']\n",
    "int_method = 'MLP' # 'MLP' or 'XGBoost' or 'RF' or 'SVM'\n",
    "xtimes = 50 \n",
    "xtimes2 = 10 \n",
    "\n",
    "feature_selection_per_network = [False]*len(feature_networks_integration)\n",
    "top_features_per_network = [50, 50]\n",
    "optional_feat_selection = False\n",
    "boruta_runs = 100\n",
    "boruta_top_features = 50\n",
    "\n",
    "max_epochs = 500\n",
    "min_epochs = 200\n",
    "patience = 30\n",
    "learning_rates = [0.01, 0.001, 0.0001]\n",
    "#learning_rates = [0.0001]\n",
    "# hid_sizes = [16, 32, 64, 128, 256, 512] \n",
    "hid_sizes = [512] \n",
    "random_state = 404"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOGAT is setting up!\n"
     ]
    }
   ],
   "source": [
    "# MOGAT run\n",
    "print('MOGAT is setting up!')\n",
    "from lib import function\n",
    "import time, io\n",
    "import os, pyreadr, itertools\n",
    "import pickle as pickle\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import statistics\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import argparse\n",
    "import errno\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ((True in feature_selection_per_network) or (optional_feat_selection == True)):\n",
    "    import rpy2\n",
    "    import rpy2.robjects as robjects\n",
    "    from rpy2.robjects.packages import importr\n",
    "    utils = importr('utils')\n",
    "    rFerns = importr('rFerns')\n",
    "    Boruta = importr('Boruta')\n",
    "    pracma = importr('pracma')\n",
    "    dplyr = importr('dplyr')\n",
    "    import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parser\n",
    "parser = argparse.ArgumentParser(description='''An integrative node classification framework, called MOGAT \n",
    "(a cancer subtype prediction methodology), that utilizes graph attentions on multiple datatype-specific networks that are annotated with multiomics datasets as node features. \n",
    "This framework is model-agnostic and could be applied to any classification problem with properly processed datatypes and networks.\n",
    "In our work, MOGAT was applied specifically to the breast cancer subtype prediction problem by applying attentions on patient similarity networks\n",
    "constructed based on multiple biological datasets from breast tumor samples.''')\n",
    "parser.add_argument('-data', \"--data_location\", nargs = 1, default = ['data(CRC)/训练'])\n",
    "parser.add_argument(\"--verbosity\", help=\"increase output verbosity\")\n",
    "args = parser.parse_args(args=[])\n",
    "dataset_name = args.data_location[0]\n",
    "\n",
    "path = base_path  + dataset_name\n",
    "if not os.path.exists(path):\n",
    "    raise FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT), path)\n",
    "        \n",
    "device = torch.device('cpu')  # 将设备设置为 CPU\n",
    "torch.set_default_tensor_type('torch.FloatTensor')  # 设置默认张量类型为 CPU 浮点张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CPU_Unpickler(pickle.Unpickler):\n",
    "    def find_class(self, module, name):\n",
    "        if module == 'torch.storage' and name == '_load_from_bytes':\n",
    "            return lambda b: torch.load(io.BytesIO(b), map_location='cpu')\n",
    "        else: return super().find_class(module, name)\n",
    "\n",
    "class Data:\n",
    "    def __init__(self, x=None, y=None):\n",
    "        self.x=x\n",
    "        self.y=y\n",
    "\n",
    "    def cpu(self):\n",
    "        self.x = self.x.cpu()\n",
    "        self.y = self.y.cpu()\n",
    "\n",
    "if ((True in feature_selection_per_network) or (optional_feat_selection == True)):\n",
    "    import rpy2\n",
    "    import rpy2.robjects as robjects\n",
    "    from rpy2.robjects.packages import importr\n",
    "    utils = importr('utils')\n",
    "    rFerns = importr('rFerns')\n",
    "    Boruta = importr('Boruta')\n",
    "    pracma = importr('pracma')\n",
    "    dplyr = importr('dplyr')\n",
    "    import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: data(CRC)/训练/labels.csv\n",
      "MOGAT is running..\n"
     ]
    }
   ],
   "source": [
    "path = base_path  + dataset_name\n",
    "if not os.path.exists(path):\n",
    "    raise FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT), path)\n",
    "        \n",
    "device = torch.device('cpu')\n",
    "#torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "#torch.cuda.set_device(6)\n",
    "\n",
    "\n",
    "data_path_node =  base_path + dataset_name +'/'\n",
    "run_name = 'MOGAT_'+  dataset_name + '_results_1'\n",
    "save_path = base_path + run_name + '/'\n",
    "excel_file = save_path + \"MOGAT_results.xlsx\"\n",
    "\n",
    "if not os.path.exists(base_path + run_name):\n",
    "    os.makedirs(base_path + run_name + '/')\n",
    "\n",
    "# 读取 CSV 文件\n",
    "file = base_path + dataset_name + '/labels.csv'  # 假设您的标签文件为 labels.csv\n",
    "print(\"Reading:\", file)\n",
    "# 使用 pandas 读取 CSV 文件\n",
    "labels_df = pd.read_csv(file)\n",
    "# 假设标签在 CSV 文件中的一列名为 'label'，您可以根据实际情况调整列名\n",
    "labels = labels_df['diagnosis'].values  # 将标签转换为 NumPy 数组\n",
    "\n",
    "file = base_path + 'data/' + dataset_name + '/mask_values.pkl'\n",
    "if os.path.exists(file):\n",
    "    with open(file, 'rb') as f:\n",
    "        train_valid_idx, test_idx = pickle.load(f)\n",
    "else:\n",
    "    train_valid_idx, test_idx= train_test_split(np.arange(len(labels)), test_size=0.20, shuffle=True, stratify=labels, random_state=random_state)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "is_first = 0\n",
    "\n",
    "print('MOGAT is running..')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "addFeatures = []\n",
    "t = range(len(node_networks))\n",
    "trial_combs = []\n",
    "for r in range(1, len(t) + 1):\n",
    "    trial_combs.extend([list(x) for x in itertools.combinations(t, r)])\n",
    "new_trial_combs = []\n",
    "for set1 in trial_combs:\n",
    "    new_trial_combs.append(list(set1))\n",
    "trial_combs = new_trial_combs\n",
    "\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models and results saved successfully.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import joblib  # 用于保存模型\n",
    "import pandas as pd\n",
    "\n",
    "# 设定交叉验证参数\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# 创建一个空的 DataFrame 来保存结果\n",
    "results_df = pd.DataFrame(columns=['Trial', 'Fold', 'True Labels', 'Predicted Labels', 'Sample Indices', 'Accuracy', 'F1 Score'])\n",
    "\n",
    "for trials in range(len(trial_combs)):\n",
    "    node_networks2 = [node_networks[i] for i in trial_combs[trials]]\n",
    "    netw_base = node_networks2[0]\n",
    "    emb_file = save_path + 'Emb_' + netw_base + '.pkl'\n",
    "    \n",
    "    with open(emb_file, 'rb') as f:\n",
    "        emb = CPU_Unpickler(f).load()\n",
    "    emb = emb.cpu()\n",
    "    \n",
    "    if len(node_networks2) > 1:\n",
    "        for netw_base in node_networks2[1:]:\n",
    "            emb_file = save_path + 'Emb_' + netw_base + '.pkl'\n",
    "            with open(emb_file, 'rb') as f:\n",
    "               cur_emb = CPU_Unpickler(f).load()\n",
    "               cur_emb = cur_emb.cpu()\n",
    "            emb = torch.cat((emb, cur_emb), dim=1)\n",
    "    \n",
    "    emb = emb.cpu()\n",
    "    \n",
    "    # 处理特征和标签\n",
    "    # ... (省略特征处理的代码，保持不变) ...\n",
    "    \n",
    "    labels_tensor = torch.tensor(labels, device=device)\n",
    "    data = Data(x=emb, y=labels_tensor)\n",
    "    data.cpu()\n",
    "    \n",
    "    # 创建训练和测试掩码\n",
    "    train_mask = np.array([i in set(train_valid_idx) for i in range(data.x.shape[0])])\n",
    "    data.train_mask = torch.tensor(train_mask, device=device)\n",
    "    test_mask = np.array([i in set(test_idx) for i in range(data.x.shape[0])])\n",
    "    data.test_mask = torch.tensor(test_mask, device=device)\n",
    "    \n",
    "    X = pd.DataFrame(data.x.numpy())\n",
    "    y = pd.DataFrame(data.y.numpy()).values.ravel()\n",
    "    \n",
    "    # 开始五倍交叉验证\n",
    "    for fold, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "        X_train_fold, X_test_fold = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train_fold, y_test_fold = y[train_index], y[test_index]\n",
    "        \n",
    "        if int_method == 'MLP':\n",
    "            params = {'hidden_layer_sizes': [ (64, 32)],\n",
    "                    'learning_rate_init': [0.001],\n",
    "                    'max_iter': [ 1500],\n",
    "                    'n_iter_no_change': [100]}\n",
    "            search = RandomizedSearchCV(estimator = MLPClassifier(solver = 'adam', activation = 'relu', early_stopping = True), \n",
    "                                        return_train_score = True, scoring = 'f1_macro', \n",
    "                                        param_distributions = params, cv = 4, n_iter = xtimes, verbose = 0)\n",
    "            search.fit(X_train_fold, y_train_fold)\n",
    "            model = MLPClassifier(solver = 'adam', activation = 'relu', early_stopping = True,\n",
    "                                max_iter = search.best_params_['max_iter'], \n",
    "                                n_iter_no_change = search.best_params_['n_iter_no_change'],\n",
    "                                hidden_layer_sizes = search.best_params_['hidden_layer_sizes'],\n",
    "                                learning_rate_init = search.best_params_['learning_rate_init'])\n",
    "        elif int_method == 'XGBoost':\n",
    "            # ... (XGBoost模型的参数设置和训练) ...\n",
    "            model = XGBClassifier(...)  # 使用最佳参数初始化模型\n",
    "        elif int_method == 'RF':\n",
    "            # ... (随机森林模型的参数设置和训练) ...\n",
    "            model = RandomForestClassifier(...)  # 使用最佳参数初始化模型\n",
    "        elif int_method == 'SVM':\n",
    "            # ... (SVM模型的参数设置和训练) ...\n",
    "            model = SVC(...)  # 使用最佳参数初始化模型\n",
    "        \n",
    "        # 训练模型\n",
    "        model.fit(X_train_fold, y_train_fold)\n",
    "        \n",
    "        # 进行预测\n",
    "        y_pred_fold = model.predict(X_test_fold)\n",
    "        \n",
    "        # 计算准确率和 F1 分数\n",
    "        accuracy = accuracy_score(y_test_fold, y_pred_fold)\n",
    "        f1 = f1_score(y_test_fold, y_pred_fold, average='macro')\n",
    "        # 保存结果\n",
    "        results_to_append = []\n",
    "        for idx, (true_label, pred_label) in enumerate(zip(y_test_fold, y_pred_fold)):\n",
    "            results_to_append.append({\n",
    "                'Trial': trials,\n",
    "                'Fold': fold,\n",
    "                'True Labels': true_label,\n",
    "                'Predicted Labels': pred_label,\n",
    "                'Sample Indices': test_index[idx],\n",
    "                'Accuracy': accuracy,\n",
    "                'F1 Score': f1\n",
    "            })\n",
    "        # 使用 pd.DataFrame() 创建新的 DataFrame 并与原 DataFrame 合并\n",
    "        results_df = pd.concat([results_df, pd.DataFrame(results_to_append)], ignore_index=True)\n",
    "\n",
    "        # 保存模型\n",
    "        joblib.dump(model, f'model_trial_{trials}_fold_{fold}.model')\n",
    "\n",
    "\n",
    "\n",
    "# 保存所有结果到 CSV 文件\n",
    "results_df.to_csv('data(CRC)\\\\cross_validation_results.csv', index=False)\n",
    "\n",
    "print(\"Models and results saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test shape: (369, 1024)\n",
      "       0         1         2         3         4         5         6     \\\n",
      "0 -0.150634 -0.682642  0.485043  0.228134  0.203529 -0.921121 -0.887200   \n",
      "1 -0.098627 -0.530374  0.321191  0.254006  0.153294 -0.983621 -0.874228   \n",
      "2  0.201044 -0.805119  0.140224  0.215465  0.238747 -0.923598 -0.912505   \n",
      "3  0.174804 -0.790710  0.411063  0.088463 -0.267014 -0.933273 -0.874443   \n",
      "4  0.380464 -0.787200 -0.089935  0.233919  0.023287 -0.978513 -0.901680   \n",
      "\n",
      "       7         8         9     ...      1014      1015      1016      1017  \\\n",
      "0 -0.815090 -0.939623 -1.018917  ... -0.721703 -1.166818 -0.665491 -0.823111   \n",
      "1 -0.443499 -0.865848 -1.112709  ... -0.651851 -1.274266 -0.736652 -0.896479   \n",
      "2 -1.038073 -0.995203 -0.998607  ... -0.793779 -1.209201 -0.926957 -0.919307   \n",
      "3 -0.936515 -0.952081 -0.984277  ... -0.780357 -1.032351 -0.937825 -0.843275   \n",
      "4 -0.958313 -0.947101 -0.947222  ... -0.655145 -1.141905 -0.882093 -0.947972   \n",
      "\n",
      "       1018      1019      1020      1021      1022      1023  \n",
      "0 -0.867883 -1.218487 -0.465865 -0.670407 -0.900667 -0.876686  \n",
      "1 -1.011954 -1.158020  0.166513 -0.985789 -0.957764 -0.909660  \n",
      "2 -1.004348 -1.231004  0.024776 -1.223562 -0.978686 -0.984221  \n",
      "3 -0.877138 -1.118297 -0.297872 -1.233825 -0.889316 -0.882891  \n",
      "4 -0.999120 -1.226102  0.268347 -1.015354 -0.959714 -0.937739  \n",
      "\n",
      "[5 rows x 1024 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 假设三个 CSV 文件的路径\n",
    "file1 = 'data(CRC)\\\\外部测试\\\\path.csv'\n",
    "file2 = 'data(CRC)\\\\外部测试\\\\cell.csv'\n",
    "file3 = 'MOGAT_data(CRC)\\\\训练_results_1\\\\Output_path.csv'\n",
    "file4='MOGAT_data(CRC)\\\\训练_results_1\\\\Output_cell.csv'\n",
    "# 读取 CSV 文件\n",
    "df1 = pd.read_csv(file1)\n",
    "df2 = pd.read_csv(file2)\n",
    "df3 = pd.read_csv(file3)\n",
    "df4 = pd.read_csv(file4)\n",
    "# 将三个 DataFrame 的列整合到一起\n",
    "X_test = pd.concat([df3,df4], axis=1)\n",
    "X_test.columns = range(1024)\n",
    "# 输出整合后的 X_test 的形状和内容\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(X_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: data(CRC)\\外部测试\\labels.csv\n",
      "MOGAT is running..\n"
     ]
    }
   ],
   "source": [
    "path = base_path  + dataset_name\n",
    "if not os.path.exists(path):\n",
    "    raise FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT), path)\n",
    "        \n",
    "device = torch.device('cpu')\n",
    "#torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "#torch.cuda.set_device(6)\n",
    "\n",
    "\n",
    "data_path_node =  base_path + dataset_name +'/'\n",
    "run_name = 'MOGAT_'+  dataset_name + '_results_1'\n",
    "save_path = base_path + run_name + '/'\n",
    "excel_file = save_path + \"MOGAT_results.xlsx\"\n",
    "\n",
    "if not os.path.exists(base_path + run_name):\n",
    "    os.makedirs(base_path + run_name + '/')\n",
    "\n",
    "# 读取 CSV 文件\n",
    "file = 'data(CRC)\\\\外部测试\\\\labels.csv'  # 假设您的标签文件为 labels.csv\n",
    "print(\"Reading:\", file)\n",
    "# 使用 pandas 读取 CSV 文件\n",
    "labels_df = pd.read_csv(file)\n",
    "# 假设标签在 CSV 文件中的一列名为 'label'，您可以根据实际情况调整列名\n",
    "labels = labels_df['diagnosis'].values  # 将标签转换为 NumPy 数组\n",
    "\n",
    "file = base_path + dataset_name + '/mask_values.pkl'\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "is_first = 0\n",
    "\n",
    "print('MOGAT is running..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "# 2. 加载保存的模型\n",
    "model_filename = 'model_trial_2_fold_4.model'  # 修改最后0，1，2，3，4\n",
    "model = joblib.load(model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_tensor = torch.tensor(labels, device=device)  # 将 labels 转换为张量\n",
    "\n",
    "data = Data(x=X_test, y=labels_tensor)  # 使用张量创建 Data 对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.DataFrame(data.x)\n",
    "y_test = pd.DataFrame(data.y).values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 使用训练好的模型进行预测\n",
    "predictions_external = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5203\n",
      "F1 Score: 0.4909\n"
     ]
    }
   ],
   "source": [
    "predictions_external = pd.DataFrame(predictions_external)\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "result_df = pd.concat([labels_df, predictions_external], axis=1)\n",
    "\n",
    "\n",
    "# 计算准确率和 F1 分数\n",
    "# 假设 labels_df 的列名为 'True Labels'，而 predictions_external 的列名为 'Predicted Labels'\n",
    "true_labels = labels_df['diagnosis']  # 替换为真实标签的列名\n",
    "predicted_labels = predictions_external.iloc[:, 0]  # 替换为预测结果的列名\n",
    "\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "f1 = f1_score(true_labels, predicted_labels, average='macro')  # 使用 'macro' 平均方式\n",
    "\n",
    "# 将结果保存到 CSV 文件\n",
    "result_df.to_csv('data(CRC)\\\\\\外部测试\\\\test_results_5.csv', index=False)###修改1，2，3，4，5\n",
    "\n",
    "# 打印准确率和 F1 分数\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
