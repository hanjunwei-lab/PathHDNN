{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Options\n",
    "addRawFeat = True\n",
    "base_path = ''\n",
    "feature_networks_integration = ['cna','exp','mut']\n",
    "#feature_networks_integration = [ 'exp']\n",
    "#feature_networks_integration = [ 'exp']\n",
    "node_networks = ['cna','exp','mut']\n",
    "#node_networks = ['exp']\n",
    "int_method = 'MLP' # 'MLP' or 'XGBoost' or 'RF' or 'SVM'\n",
    "xtimes = 50 \n",
    "xtimes2 = 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_selection_per_network = [False]*len(feature_networks_integration)\n",
    "top_features_per_network = [50, 50,50]\n",
    "optional_feat_selection = False\n",
    "boruta_runs = 100\n",
    "boruta_top_features = 50\n",
    "\n",
    "max_epochs = 500\n",
    "min_epochs = 200\n",
    "patience = 30\n",
    "learning_rates = [0.01, 0.001, 0.0001]\n",
    "#learning_rates = [0.0001]\n",
    "# hid_sizes = [16, 32, 64, 128, 256, 512] \n",
    "hid_sizes = [512] \n",
    "random_state = 404"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOGAT is setting up!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Miniconda3\\envs\\deep\\lib\\site-packages\\torch_cluster\\nearest.py:3: UserWarning: A NumPy version >=1.23.5 and <2.3.0 is required for this version of SciPy (detected version 1.23.2)\n",
      "  import scipy.cluster\n"
     ]
    }
   ],
   "source": [
    "# MOGAT run\n",
    "print('MOGAT is setting up!')\n",
    "from lib import module2, function\n",
    "import time\n",
    "import os, pyreadr, itertools\n",
    "import pickle as pickle#改\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import statistics\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "import os\n",
    "import torch\n",
    "import argparse\n",
    "import errno\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ((True in feature_selection_per_network) or (optional_feat_selection == True)):\n",
    "    import rpy2\n",
    "    import rpy2.robjects as robjects\n",
    "    from rpy2.robjects.packages import importr\n",
    "    utils = importr('utils')\n",
    "    rFerns = importr('rFerns')\n",
    "    Boruta = importr('Boruta')\n",
    "    pracma = importr('pracma')\n",
    "    dplyr = importr('dplyr')\n",
    "    import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parser\n",
    "parser = argparse.ArgumentParser(description='''An integrative node classification framework, called MOGAT \n",
    "(a cancer subtype prediction methodology), that utilizes graph attentions on multiple datatype-specific networks that are annotated with multiomics datasets as node features. \n",
    "This framework is model-agnostic and could be applied to any classification problem with properly processed datatypes and networks.\n",
    "In our work, MOGAT was applied specifically to the breast cancer subtype prediction problem by applying attentions on patient similarity networks\n",
    "constructed based on multiple biological datasets from breast tumor samples.''')\n",
    "parser.add_argument('-data', \"--data_location\", nargs = 1, default = ['data(li)delete10/训练'])\n",
    "parser.add_argument(\"--verbosity\", help=\"increase output verbosity\")\n",
    "args = parser.parse_args(args=[])\n",
    "dataset_name = args.data_location[0]\n",
    "\n",
    "path = base_path  + dataset_name\n",
    "if not os.path.exists(path):\n",
    "    raise FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT), path)\n",
    "        \n",
    "device = torch.device('cpu')  # 将设备设置为 CPU\n",
    "torch.set_default_tensor_type('torch.FloatTensor')  # 设置默认张量类型为 CPU 浮点张量\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data(li)delete10/训练\n"
     ]
    }
   ],
   "source": [
    "print(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out, emb1 = model(data)\n",
    "    # print(\"device\",out.get_device(),emb1.get_device(), data.get_device())\n",
    "    train_idx = data.train_mask\n",
    "    # train_idx.cpu()\n",
    "    obj1 = out[train_idx]\n",
    "    # print(data.y.get_device())\n",
    "    # print(train_idx.get_device())\n",
    "    obj2 = data.y[train_idx]\n",
    "    loss = criterion(obj1, obj2)\n",
    "    # train_idx.cuda()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return emb1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out, emb2 = model(data)\n",
    "        pred = out.argmax(dim=1)\n",
    "        valid_idx = data.valid_mask\n",
    "        # valid_idx.cpu()\n",
    "        loss = criterion(out[valid_idx], data.y[valid_idx])\n",
    "        # valid_idx.cuda()       \n",
    "    return loss, emb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "data_path_node =  base_path + dataset_name +'/'\n",
    "run_name = 'MOGAT_'+  dataset_name + '_results_1'\n",
    "save_path = base_path + run_name + '/'\n",
    "excel_file = save_path + \"MOGAT_results.xlsx\"\n",
    "\n",
    "if not os.path.exists(base_path + run_name):\n",
    "    os.makedirs(base_path + run_name + '/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: data(li)delete10/训练/labels.csv\n",
      "MOGAT is running..\n"
     ]
    }
   ],
   "source": [
    "# 读取 CSV 文件\n",
    "file = base_path + dataset_name + '/labels.csv'  # 假设您的标签文件为 labels.csv\n",
    "print(\"Reading:\", file)\n",
    "# 使用 pandas 读取 CSV 文件\n",
    "labels_df = pd.read_csv(file)\n",
    "# 假设标签在 CSV 文件中的一列名为 'label'，您可以根据实际情况调整列名\n",
    "labels = labels_df['diagnosis'].values  # 将标签转换为 NumPy 数组\n",
    "#file = base_path + dataset_name +'/labels.pkl'\n",
    "#print(\"Reading:\", file)\n",
    "#with open(file, 'rb') as f:\n",
    "    #labels = pickle.load(f)\n",
    "\n",
    "file = base_path+ dataset_name + '/mask_values.pkl'\n",
    "if os.path.exists(file):\n",
    "    with open(file, 'rb') as f:\n",
    "        train_valid_idx, test_idx = pickle.load(f)\n",
    "else:\n",
    "    train_valid_idx, test_idx= train_test_split(np.arange(len(labels)), test_size=0.20, shuffle=True, stratify=labels, random_state=random_state)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "is_first = 0\n",
    "\n",
    "print('MOGAT is running..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: data(li)delete10/训练/cna.csv\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data(li)delete10/训练/cna.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReading:\u001b[39m\u001b[38;5;124m\"\u001b[39m, file)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# 使用 pandas 读取 CSV 文件\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m feat_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# 如果需要将 DataFrame 转换为 NumPy 数组，可以使用 .values 或 .to_numpy()\u001b[39;00m\n\u001b[0;32m      7\u001b[0m feat \u001b[38;5;241m=\u001b[39m feat_df\u001b[38;5;241m.\u001b[39mvalues  \u001b[38;5;66;03m# 将特征转换为 NumPy 数组\u001b[39;00m\n",
      "File \u001b[1;32md:\\Miniconda3\\envs\\deep\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Miniconda3\\envs\\deep\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32md:\\Miniconda3\\envs\\deep\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Miniconda3\\envs\\deep\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32md:\\Miniconda3\\envs\\deep\\lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data(li)delete10/训练/cna.csv'"
     ]
    }
   ],
   "source": [
    "for netw in node_networks:\n",
    "    file = base_path + dataset_name + '/' + netw + '.csv'  # 假设您的特征文件为 netw.csv\n",
    "    print(\"Reading:\", file)\n",
    "    # 使用 pandas 读取 CSV 文件\n",
    "    feat_df = pd.read_csv(file)\n",
    "    # 如果需要将 DataFrame 转换为 NumPy 数组，可以使用 .values 或 .to_numpy()\n",
    "    feat = feat_df.values  # 将特征转换为 NumPy 数组\n",
    "    #file = base_path + 'data/' + dataset_name +'/'+ netw +'.pkl'\n",
    "    #print(\"Reading:\",file)\n",
    "    #with open(file, 'rb') as f:\n",
    "    #feat = pickle.load(f)\n",
    "    if feature_selection_per_network[node_networks.index(netw)] and top_features_per_network[node_networks.index(netw)] < feat.values.shape[1]:     \n",
    "        feat_flat = [item for sublist in feat.values.tolist() for item in sublist]\n",
    "        feat_temp = robjects.FloatVector(feat_flat)\n",
    "        robjects.globalenv['feat_matrix'] = robjects.r('matrix')(feat_temp)\n",
    "        robjects.globalenv['feat_x'] = robjects.IntVector(feat.shape)\n",
    "        robjects.globalenv['labels_vector'] = robjects.IntVector(labels.tolist())\n",
    "        robjects.globalenv['top'] = top_features_per_network[node_networks.index(netw)]\n",
    "        robjects.globalenv['maxBorutaRuns'] = boruta_runs\n",
    "        robjects.r('''\n",
    "            require(rFerns)\n",
    "            require(Boruta)\n",
    "            labels_vector = as.factor(labels_vector)\n",
    "            feat_matrix <- Reshape(feat_matrix, feat_x[1])\n",
    "            feat_data = data.frame(feat_matrix)\n",
    "            colnames(feat_data) <- 1:feat_x[2]\n",
    "            feat_data <- feat_data %>%\n",
    "                mutate('Labels' = labels_vector)\n",
    "            boruta.train <- Boruta(feat_data$Labels ~ ., data= feat_data, doTrace = 0, getImp=getImpFerns, holdHistory = T, maxRuns = maxBorutaRuns)\n",
    "            thr = sort(attStats(boruta.train)$medianImp, decreasing = T)[top]\n",
    "            boruta_signif = rownames(attStats(boruta.train)[attStats(boruta.train)$medianImp >= thr,])\n",
    "                ''')\n",
    "        boruta_signif = robjects.globalenv['boruta_signif']\n",
    "        robjects.r.rm(\"feat_matrix\")\n",
    "        robjects.r.rm(\"labels_vector\")\n",
    "        robjects.r.rm(\"feat_data\")\n",
    "        robjects.r.rm(\"boruta_signif\")\n",
    "        robjects.r.rm(\"thr\")\n",
    "        topx = []\n",
    "        for index in boruta_signif:\n",
    "            t_index=re.sub(\"`\",\"\",index)\n",
    "            topx.append((np.array(feat.values).T)[int(t_index)-1])\n",
    "        topx = np.array(topx)\n",
    "        values = torch.tensor(topx.T, device=device)\n",
    "    elif feature_selection_per_network[node_networks.index(netw)] and top_features_per_network[node_networks.index(netw)] >= feat.values.shape[1]:\n",
    "        values = feat\n",
    "    else:\n",
    "        values = feat\n",
    "\n",
    "if is_first == 0:\n",
    "    new_x = torch.tensor(values, device=device).float()\n",
    "    is_first = 1\n",
    "else:\n",
    "    new_x = torch.cat((new_x, torch.tensor(values, device=device).float()), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data(CRC)/训练/edges_path.csv\n",
      "GAT trained for hyperparameter: learning rate 0.01 hidden layer size 512\n",
      "GAT trained for hyperparameter: learning rate 0.01 hidden layer size 512\n",
      "GAT trained for hyperparameter: learning rate 0.01 hidden layer size 512\n",
      "GAT trained for hyperparameter: learning rate 0.01 hidden layer size 512\n",
      "GAT trained for hyperparameter: learning rate 0.01 hidden layer size 512\n",
      "GAT trained for hyperparameter: learning rate 0.01 hidden layer size 512\n",
      "GAT trained for hyperparameter: learning rate 0.01 hidden layer size 512\n",
      "GAT trained for hyperparameter: learning rate 0.01 hidden layer size 512\n",
      "GAT trained for hyperparameter: learning rate 0.01 hidden layer size 512\n",
      "GAT trained for hyperparameter: learning rate 0.01 hidden layer size 512\n",
      "GAT trained for hyperparameter: learning rate 0.001 hidden layer size 512\n",
      "GAT trained for hyperparameter: learning rate 0.001 hidden layer size 512\n",
      "GAT trained for hyperparameter: learning rate 0.001 hidden layer size 512\n",
      "GAT trained for hyperparameter: learning rate 0.001 hidden layer size 512\n",
      "GAT trained for hyperparameter: learning rate 0.001 hidden layer size 512\n",
      "GAT trained for hyperparameter: learning rate 0.001 hidden layer size 512\n",
      "GAT trained for hyperparameter: learning rate 0.001 hidden layer size 512\n",
      "GAT trained for hyperparameter: learning rate 0.001 hidden layer size 512\n",
      "GAT trained for hyperparameter: learning rate 0.001 hidden layer size 512\n",
      "GAT trained for hyperparameter: learning rate 0.001 hidden layer size 512\n",
      "GAT trained for hyperparameter: learning rate 0.0001 hidden layer size 512\n",
      "GAT trained for hyperparameter: learning rate 0.0001 hidden layer size 512\n",
      "GAT trained for hyperparameter: learning rate 0.0001 hidden layer size 512\n",
      "GAT trained for hyperparameter: learning rate 0.0001 hidden layer size 512\n",
      "GAT trained for hyperparameter: learning rate 0.0001 hidden layer size 512\n",
      "GAT trained for hyperparameter: learning rate 0.0001 hidden layer size 512\n",
      "GAT trained for hyperparameter: learning rate 0.0001 hidden layer size 512\n",
      "GAT trained for hyperparameter: learning rate 0.0001 hidden layer size 512\n",
      "GAT trained for hyperparameter: learning rate 0.0001 hidden layer size 512\n",
      "GAT trained for hyperparameter: learning rate 0.0001 hidden layer size 512\n",
      "GAT training Started 2\n",
      "GAT training done for data(CRC)/训练/edges_path.pkl\n",
      "Reading data(CRC)/训练/edges_cell.csv\n",
      "GAT trained for hyperparameter: learning rate 0.01 hidden layer size 512\n",
      "GAT trained for hyperparameter: learning rate 0.01 hidden layer size 512\n",
      "GAT trained for hyperparameter: learning rate 0.01 hidden layer size 512\n",
      "GAT trained for hyperparameter: learning rate 0.01 hidden layer size 512\n",
      "GAT trained for hyperparameter: learning rate 0.01 hidden layer size 512\n",
      "GAT trained for hyperparameter: learning rate 0.01 hidden layer size 512\n",
      "GAT trained for hyperparameter: learning rate 0.01 hidden layer size 512\n",
      "GAT trained for hyperparameter: learning rate 0.01 hidden layer size 512\n",
      "GAT trained for hyperparameter: learning rate 0.01 hidden layer size 512\n",
      "GAT trained for hyperparameter: learning rate 0.01 hidden layer size 512\n",
      "GAT trained for hyperparameter: learning rate 0.001 hidden layer size 512\n",
      "GAT trained for hyperparameter: learning rate 0.001 hidden layer size 512\n",
      "GAT trained for hyperparameter: learning rate 0.001 hidden layer size 512\n",
      "GAT trained for hyperparameter: learning rate 0.001 hidden layer size 512\n",
      "GAT trained for hyperparameter: learning rate 0.001 hidden layer size 512\n",
      "GAT trained for hyperparameter: learning rate 0.001 hidden layer size 512\n",
      "GAT trained for hyperparameter: learning rate 0.001 hidden layer size 512\n",
      "GAT trained for hyperparameter: learning rate 0.001 hidden layer size 512\n",
      "GAT trained for hyperparameter: learning rate 0.001 hidden layer size 512\n",
      "GAT trained for hyperparameter: learning rate 0.001 hidden layer size 512\n",
      "GAT trained for hyperparameter: learning rate 0.0001 hidden layer size 512\n",
      "GAT trained for hyperparameter: learning rate 0.0001 hidden layer size 512\n",
      "GAT trained for hyperparameter: learning rate 0.0001 hidden layer size 512\n",
      "GAT trained for hyperparameter: learning rate 0.0001 hidden layer size 512\n",
      "GAT trained for hyperparameter: learning rate 0.0001 hidden layer size 512\n",
      "GAT trained for hyperparameter: learning rate 0.0001 hidden layer size 512\n",
      "GAT trained for hyperparameter: learning rate 0.0001 hidden layer size 512\n",
      "GAT trained for hyperparameter: learning rate 0.0001 hidden layer size 512\n",
      "GAT trained for hyperparameter: learning rate 0.0001 hidden layer size 512\n",
      "GAT trained for hyperparameter: learning rate 0.0001 hidden layer size 512\n",
      "GAT training Started 2\n",
      "GAT training done for data(CRC)/训练/edges_cell.pkl\n"
     ]
    }
   ],
   "source": [
    "for n in range(len(node_networks)):\n",
    "    netw_base = node_networks[n]\n",
    "    # 假设 netw_base 是网络名称，data_path_node 是数据路径\n",
    "    csv_file_path = data_path_node + 'edges_' + netw_base + '.csv'\n",
    "\n",
    "    # 读取 CSV 文件\n",
    "    print(\"Reading\", csv_file_path)\n",
    "    edge_index = pd.read_csv(csv_file_path)\n",
    "\n",
    "    #with open(data_path_node + 'edges_' + netw_base + '.pkl', 'rb') as f:\n",
    "        #print(\"Reading\",data_path_node + 'edges_' + netw_base + '.pkl' )\n",
    "        #edge_index = pickle.load(f)\n",
    "    best_ValidLoss = np.Inf\n",
    "\n",
    "    for learning_rate in learning_rates:\n",
    "        for hid_size in hid_sizes:\n",
    "            av_valid_losses = list()\n",
    "\n",
    "            for ii in range(xtimes2):\n",
    "                data = Data(x=new_x, edge_index=torch.tensor(edge_index[edge_index.columns[0:2]].transpose().values, device=device).long(),\n",
    "                            edge_attr=torch.tensor(edge_index[edge_index.columns[2]].transpose().values, device=device).float(), y=labels) \n",
    "                #data.cuda()\n",
    "                if isinstance(data.y, np.ndarray):\n",
    "                  data.y = torch.tensor(data.y, device=device)\n",
    "                  data.x = torch.tensor(data.x, device=device)\n",
    "                X = data.x[train_valid_idx]\n",
    "                y = data.y[train_valid_idx]\n",
    "                rskf = RepeatedStratifiedKFold(n_splits=4, n_repeats=1)\n",
    "\n",
    "                for train_part, valid_part in rskf.split(X, y):\n",
    "                    train_idx = train_valid_idx[train_part]\n",
    "                    valid_idx = train_valid_idx[valid_part]\n",
    "                    break\n",
    "\n",
    "                train_mask = np.array([i in set(train_idx) for i in range(data.x.shape[0])])\n",
    "                valid_mask = np.array([i in set(valid_idx) for i in range(data.x.shape[0])])\n",
    "                data.valid_mask = torch.tensor(valid_mask, device=device)\n",
    "                data.train_mask = torch.tensor(train_mask, device=device)\n",
    "                test_mask = np.array([i in set(test_idx) for i in range(data.x.shape[0])])\n",
    "                data.test_mask = torch.tensor(test_mask, device=device)\n",
    "\n",
    "                in_size = data.x.shape[1]\n",
    "                out_size = torch.unique(data.y).shape[0]\n",
    "\n",
    "                print(\"GAT trained for hyperparameter: learning rate\", learning_rate, \"hidden layer size\", hid_size)\n",
    "                model = module2.Net(in_size=in_size, hid_size=hid_size, out_size=out_size)\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "                min_valid_loss = np.Inf\n",
    "                patience_count = 0\n",
    "                data.cpu()\n",
    "                for epoch in range(max_epochs):\n",
    "                    emb = train()\n",
    "                    this_valid_loss, emb = validate()\n",
    "                    # print(\"Epoch:\",epoch, \"val_loss\",this_valid_loss)\n",
    "\n",
    "                    if this_valid_loss < min_valid_loss:\n",
    "                        min_valid_loss = this_valid_loss\n",
    "                        patience_count = 0\n",
    "                    else:\n",
    "                        patience_count += 1\n",
    "\n",
    "                    if epoch >= min_epochs and patience_count >= patience:\n",
    "                        break\n",
    "\n",
    "                av_valid_losses.append(min_valid_loss.item())\n",
    "\n",
    "            av_valid_loss = round(statistics.median(av_valid_losses), 3)\n",
    "            \n",
    "            if av_valid_loss < best_ValidLoss:\n",
    "                best_ValidLoss = av_valid_loss\n",
    "                best_emb_lr = learning_rate\n",
    "                best_emb_hs = hid_size\n",
    "                \n",
    "    data = Data(x=new_x, edge_index=torch.tensor(edge_index[edge_index.columns[0:2]].transpose().values, device=device).long(),\n",
    "                edge_attr=torch.tensor(edge_index[edge_index.columns[2]].transpose().values, device=device).float(), y=labels) \n",
    "    data.cpu()\n",
    "        # 确保 data.y 是一个 PyTorch 张量\n",
    "    if isinstance(data.y, np.ndarray):\n",
    "        data.y = torch.tensor(data.y, device=device)\n",
    "        # 确保 data.y 是一个 PyTorch 张量\n",
    "    if isinstance(data.y, np.ndarray):\n",
    "        data.x = torch.tensor(data.x, device=device)\n",
    "\n",
    "    X = data.x[train_valid_idx]\n",
    "    y = data.y[train_valid_idx]\n",
    "    \n",
    "    train_mask = np.array([i in set(train_valid_idx) for i in range(data.x.shape[0])])\n",
    "    data.train_mask = torch.tensor(train_mask, device=device)\n",
    "    valid_mask = np.array([i in set(test_idx) for i in range(data.x.shape[0])])\n",
    "    data.valid_mask = torch.tensor(valid_mask, device=device)\n",
    "    \n",
    "    in_size = data.x.shape[1]\n",
    "    out_size = torch.unique(data.y).shape[0]\n",
    "    \n",
    "    # initializing GAT\n",
    "    print(\"GAT training Started 2\")\n",
    "    model = module2.Net(in_size=in_size, hid_size=best_emb_hs, out_size=out_size)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=best_emb_lr)\n",
    "\n",
    "    min_valid_loss = np.Inf\n",
    "    patience_count = 0\n",
    "                \n",
    "    for epoch in range(max_epochs):\n",
    "        emb = train()\n",
    "        this_valid_loss, emb = validate()\n",
    "        # print(\"Epoch:\",epoch, \"val_loss\",this_valid_loss)\n",
    "\n",
    "        if this_valid_loss < min_valid_loss:\n",
    "            min_valid_loss = this_valid_loss\n",
    "            patience_count = 0\n",
    "            selected_emb = emb\n",
    "        else:\n",
    "            patience_count += 1\n",
    "\n",
    "        if epoch >= min_epochs and patience_count >= patience:\n",
    "            break\n",
    "\n",
    "\n",
    "    emb_file = save_path + 'Emb_' +  netw_base + '.pkl'\n",
    "    with open(emb_file, 'wb') as f:\n",
    "        pickle.dump(selected_emb, f)\n",
    "        pd.DataFrame(selected_emb).to_csv(emb_file[:-4] + '.csv')\n",
    "    print(\"GAT training done for\", data_path_node + 'edges_' + netw_base + '.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 474.3 seconds for all runs.\n",
      "MOGAT is done.\n",
      "Results are available at MOGAT_data(CRC)/训练_results_1/MOGAT_results.xlsx\n"
     ]
    }
   ],
   "source": [
    "end = time.time()\n",
    "print('It took ' + str(round(end - start, 1)) + ' seconds for all runs.')\n",
    "print('MOGAT is done.')\n",
    "print('Results are available at ' + excel_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to data(CRC)/训练/feature_model.pth\n"
     ]
    }
   ],
   "source": [
    "# 保存模型\n",
    "model_file_path = \"data(CRC)/训练/feature_model.pth\"  # 指定保存路径\n",
    "torch.save(model.state_dict(), model_file_path)\n",
    "torch.save(model,\"data(CRC)/训练/feature_model.model\")  # 保存模型的状态字典\n",
    "print(\"Model saved to\", model_file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: data(CRC)/外部测试/labels.csv\n",
      "MOGAT is running..\n"
     ]
    }
   ],
   "source": [
    "dataset_name=\"data(CRC)/外部测试\"\n",
    "# 读取 CSV 文件\n",
    "file = base_path + dataset_name + '/labels.csv'  # 假设您的标签文件为 labels.csv\n",
    "print(\"Reading:\", file)\n",
    "# 使用 pandas 读取 CSV 文件\n",
    "labels_df = pd.read_csv(file)\n",
    "# 假设标签在 CSV 文件中的一列名为 'label'，您可以根据实际情况调整列名\n",
    "labels = labels_df['diagnosis'].values  # 将标签转换为 NumPy 数组\n",
    "#file = base_path + dataset_name +'/labels.pkl'\n",
    "#print(\"Reading:\", file)\n",
    "#with open(file, 'rb') as f:\n",
    "    #labels = pickle.load(f)\n",
    "start = time.time()\n",
    "\n",
    "is_first = 0\n",
    "\n",
    "print('MOGAT is running..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: data(CRC)/外部测试/path.csv\n",
      "Reading: data(CRC)/外部测试/cell.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for netw in node_networks:\n",
    "    file = base_path + dataset_name + '/' + netw + '.csv'  # 假设您的特征文件为 netw.csv\n",
    "    print(\"Reading:\", file)\n",
    "    # 使用 pandas 读取 CSV 文件\n",
    "    feat_df = pd.read_csv(file)\n",
    "    # 如果需要将 DataFrame 转换为 NumPy 数组，可以使用 .values 或 .to_numpy()\n",
    "    feat = feat_df.values  # 将特征转换为 NumPy 数组\n",
    "    #file = base_path + 'data/' + dataset_name +'/'+ netw +'.pkl'\n",
    "    #print(\"Reading:\",file)\n",
    "    #with open(file, 'rb') as f:\n",
    "    #feat = pickle.load(f)\n",
    "    if feature_selection_per_network[node_networks.index(netw)] and top_features_per_network[node_networks.index(netw)] < feat.values.shape[1]:     \n",
    "        feat_flat = [item for sublist in feat.values.tolist() for item in sublist]\n",
    "        feat_temp = robjects.FloatVector(feat_flat)\n",
    "        robjects.globalenv['feat_matrix'] = robjects.r('matrix')(feat_temp)\n",
    "        robjects.globalenv['feat_x'] = robjects.IntVector(feat.shape)\n",
    "        robjects.globalenv['labels_vector'] = robjects.IntVector(labels.tolist())\n",
    "        robjects.globalenv['top'] = top_features_per_network[node_networks.index(netw)]\n",
    "        robjects.globalenv['maxBorutaRuns'] = boruta_runs\n",
    "        robjects.r('''\n",
    "            require(rFerns)\n",
    "            require(Boruta)\n",
    "            labels_vector = as.factor(labels_vector)\n",
    "            feat_matrix <- Reshape(feat_matrix, feat_x[1])\n",
    "            feat_data = data.frame(feat_matrix)\n",
    "            colnames(feat_data) <- 1:feat_x[2]\n",
    "            feat_data <- feat_data %>%\n",
    "                mutate('Labels' = labels_vector)\n",
    "            boruta.train <- Boruta(feat_data$Labels ~ ., data= feat_data, doTrace = 0, getImp=getImpFerns, holdHistory = T, maxRuns = maxBorutaRuns)\n",
    "            thr = sort(attStats(boruta.train)$medianImp, decreasing = T)[top]\n",
    "            boruta_signif = rownames(attStats(boruta.train)[attStats(boruta.train)$medianImp >= thr,])\n",
    "                ''')\n",
    "        boruta_signif = robjects.globalenv['boruta_signif']\n",
    "        robjects.r.rm(\"feat_matrix\")\n",
    "        robjects.r.rm(\"labels_vector\")\n",
    "        robjects.r.rm(\"feat_data\")\n",
    "        robjects.r.rm(\"boruta_signif\")\n",
    "        robjects.r.rm(\"thr\")\n",
    "        topx = []\n",
    "        for index in boruta_signif:\n",
    "            t_index=re.sub(\"`\",\"\",index)\n",
    "            topx.append((np.array(feat.values).T)[int(t_index)-1])\n",
    "        topx = np.array(topx)\n",
    "        values = torch.tensor(topx.T, device=device)\n",
    "    elif feature_selection_per_network[node_networks.index(netw)] and top_features_per_network[node_networks.index(netw)] >= feat.values.shape[1]:\n",
    "        values = feat\n",
    "    else:\n",
    "        values = feat\n",
    "\n",
    "if is_first == 0:\n",
    "    new_x = torch.tensor(values, device=device).float()\n",
    "    is_first = 1\n",
    "else:\n",
    "    new_x = torch.cat((new_x, torch.tensor(values, device=device).float()), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data(CRC)/外部测试/edges_path.csv\n",
      "GAT training done for 外部测试/edges_path.pkl\n",
      "Output saved to MOGAT_data(CRC)/训练_results_1/Output_path.pkl\n",
      "Reading data(CRC)/外部测试/edges_cell.csv\n",
      "GAT training done for 外部测试/edges_cell.pkl\n",
      "Output saved to MOGAT_data(CRC)/训练_results_1/Output_cell.pkl\n"
     ]
    }
   ],
   "source": [
    "for n in range(len(node_networks)):\n",
    "    netw_base = node_networks[n]\n",
    "    csv_file_path = dataset_name +'/'+ 'edges_' + netw_base + '.csv'\n",
    "\n",
    "    # 读取 CSV 文件\n",
    "    print(\"Reading\", csv_file_path)\n",
    "    edge_index = pd.read_csv(csv_file_path)\n",
    "\n",
    "    # 准备数据\n",
    "    data = Data(\n",
    "        x=new_x,\n",
    "        edge_index=torch.tensor(edge_index[edge_index.columns[0:2]].transpose().values, device=device).long(),\n",
    "        edge_attr=torch.tensor(edge_index[edge_index.columns[2]].transpose().values, device=device).float(),\n",
    "        y=labels\n",
    "    )\n",
    "\n",
    "    # 确保 data.y 是一个 PyTorch 张量\n",
    "    if isinstance(data.y, np.ndarray):\n",
    "        data.y = torch.tensor(data.y, device=device)\n",
    "\n",
    "    # 创建测试集掩码\n",
    "    test_mask = np.array([i in set(test_idx) for i in range(data.x.shape[0])])\n",
    "    data.test_mask = torch.tensor(test_mask, device=device)\n",
    "\n",
    "    # 这里假设您已经有一个训练好的模型\n",
    "    # 加载模型\n",
    "    in_size = data.x.shape[1]\n",
    "    out_size = torch.unique(data.y).shape[0]\n",
    "    model_file = \"data(CRC)/训练/feature_model.pth\"  # 假设您保存了模型\n",
    "    model = module2.Net(in_size=in_size, hid_size=hid_size, out_size=out_size)\n",
    "\n",
    "    model.load_state_dict(torch.load(model_file))\n",
    "    model.eval()  # 设置模型为评估模式\n",
    "\n",
    "    # 在你的代码中找到处理模型输出的部分\n",
    "    with torch.no_grad():  # 不需要计算梯度\n",
    "        out = model(data)  # 仅对测试集进行预测\n",
    "\n",
    "    # 检查模型输出的类型\n",
    "    if isinstance(out, tuple):\n",
    "        # 如果输出是一个元组，选择你需要的部分\n",
    "        # 例如，如果你只需要第一个输出，可以这样做：\n",
    "        out = out[1]  # 选择第一个元素，假设这是 logits\n",
    "    output_file = save_path + 'Output_' + netw_base + '.pkl'\n",
    "    # 然后继续处理输出\n",
    "    out_np = out.detach().cpu().numpy()  # 将张量从 GPU 移动到 CPU 并转换为 NumPy 数组\n",
    "    pd.DataFrame(out_np).to_csv(output_file[:-4] + '.csv', index=False)\n",
    "\n",
    "    print(\"GAT training done for\", '外部测试' + '/' + 'edges_' + netw_base + '.pkl')\n",
    "    print(\"Output saved to\", output_file)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Options\n",
    "addRawFeat = True\n",
    "base_path = ''\n",
    "feature_networks_integration = [ 'path','cell']\n",
    "#feature_networks_integration = [ 'exp']\n",
    "node_networks = [ 'path','cell']\n",
    "#node_networks = [ 'exp']\n",
    "int_method = 'MLP' # 'MLP' or 'XGBoost' or 'RF' or 'SVM'\n",
    "xtimes = 50 \n",
    "xtimes2 = 10 \n",
    "\n",
    "feature_selection_per_network = [False]*len(feature_networks_integration)\n",
    "top_features_per_network = [50, 50]\n",
    "optional_feat_selection = False\n",
    "boruta_runs = 100\n",
    "boruta_top_features = 50\n",
    "\n",
    "max_epochs = 500\n",
    "min_epochs = 200\n",
    "patience = 30\n",
    "learning_rates = [0.01, 0.001, 0.0001]\n",
    "#learning_rates = [0.0001]\n",
    "# hid_sizes = [16, 32, 64, 128, 256, 512] \n",
    "hid_sizes = [512] \n",
    "random_state = 404"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOGAT is setting up!\n"
     ]
    }
   ],
   "source": [
    "# MOGAT run\n",
    "print('MOGAT is setting up!')\n",
    "from lib import function\n",
    "import time, io\n",
    "import os, pyreadr, itertools\n",
    "import pickle as pickle\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import statistics\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import argparse\n",
    "import errno\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CPU_Unpickler(pickle.Unpickler):\n",
    "    def find_class(self, module, name):\n",
    "        if module == 'torch.storage' and name == '_load_from_bytes':\n",
    "            return lambda b: torch.load(io.BytesIO(b), map_location='cpu')\n",
    "        else: return super().find_class(module, name)\n",
    "\n",
    "class Data:\n",
    "    def __init__(self, x=None, y=None):\n",
    "        self.x=x\n",
    "        self.y=y\n",
    "\n",
    "    def cpu(self):\n",
    "        self.x = self.x.cpu()\n",
    "        self.y = self.y.cpu()\n",
    "\n",
    "if ((True in feature_selection_per_network) or (optional_feat_selection == True)):\n",
    "    import rpy2\n",
    "    import rpy2.robjects as robjects\n",
    "    from rpy2.robjects.packages import importr\n",
    "    utils = importr('utils')\n",
    "    rFerns = importr('rFerns')\n",
    "    Boruta = importr('Boruta')\n",
    "    pracma = importr('pracma')\n",
    "    dplyr = importr('dplyr')\n",
    "    import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: 训练/labels.csv\n",
      "MOGAT is running..\n"
     ]
    }
   ],
   "source": [
    "\n",
    "path = base_path  + dataset_name\n",
    "if not os.path.exists(path):\n",
    "    raise FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT), path)\n",
    "        \n",
    "device = torch.device('cpu')\n",
    "#torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "#torch.cuda.set_device(6)\n",
    "\n",
    "\n",
    "data_path_node =  base_path + dataset_name +'/'\n",
    "run_name = 'MOGAT_'+  dataset_name + '_results_1'\n",
    "save_path = base_path + run_name + '/'\n",
    "excel_file = save_path + \"MOGAT_results.xlsx\"\n",
    "\n",
    "if not os.path.exists(base_path + run_name):\n",
    "    os.makedirs(base_path + run_name + '/')\n",
    "\n",
    "# 读取 CSV 文件\n",
    "file = base_path + dataset_name + '/labels.csv'  # 假设您的标签文件为 labels.csv\n",
    "print(\"Reading:\", file)\n",
    "# 使用 pandas 读取 CSV 文件\n",
    "labels_df = pd.read_csv(file)\n",
    "# 假设标签在 CSV 文件中的一列名为 'label'，您可以根据实际情况调整列名\n",
    "labels = labels_df['diagnosis'].values  # 将标签转换为 NumPy 数组\n",
    "\n",
    "file = base_path + 'data/' + dataset_name + '/mask_values.pkl'\n",
    "if os.path.exists(file):\n",
    "    with open(file, 'rb') as f:\n",
    "        train_valid_idx, test_idx = pickle.load(f)\n",
    "else:\n",
    "    train_valid_idx, test_idx= train_test_split(np.arange(len(labels)), test_size=0.20, shuffle=True, stratify=labels, random_state=random_state)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "is_first = 0\n",
    "\n",
    "print('MOGAT is running..')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'node_networks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m addFeatures \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 2\u001b[0m t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mnode_networks\u001b[49m))\n\u001b[0;32m      3\u001b[0m trial_combs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(t) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'node_networks' is not defined"
     ]
    }
   ],
   "source": [
    "addFeatures = []\n",
    "t = range(len(node_networks))\n",
    "trial_combs = []\n",
    "for r in range(1, len(t) + 1):\n",
    "    trial_combs.extend([list(x) for x in itertools.combinations(t, r)])\n",
    "new_trial_combs = []\n",
    "for set1 in trial_combs:\n",
    "    new_trial_combs.append(list(set1))\n",
    "trial_combs = new_trial_combs\n",
    "\n",
    "device = torch.device('cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trial_combs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtrial_combs\u001b[49m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(trial_combs))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trial_combs' is not defined"
     ]
    }
   ],
   "source": [
    "print(trial_combs)\n",
    "range(len(trial_combs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: 训练/path.csv\n",
      "Reading: 训练/cell.csv\n",
      "-1\n",
      "-1\n",
      "Second Model Training Started\n",
      "Combination 0 ['path'] >  selected parameters = {'n_iter_no_change': 100, 'max_iter': 1500, 'learning_rate_init': 0.001, 'hidden_layer_sizes': (64, 32)}, train accuracy = 0.816+-0.015, train weighted-f1 = 0.815+-0.015, train macro-f1 = 0.798+-0.016, test accuracy = 0.736+-0.016, test weighted-f1 = 0.736+-0.014, test macro-f1 = 0.718+-0.018\n",
      "Reading: 训练/path.csv\n",
      "Reading: 训练/cell.csv\n",
      "-1\n",
      "-1\n",
      "Second Model Training Started\n",
      "Combination 1 ['cell'] >  selected parameters = {'n_iter_no_change': 100, 'max_iter': 1500, 'learning_rate_init': 0.001, 'hidden_layer_sizes': (64, 32)}, train accuracy = 0.864+-0.031, train weighted-f1 = 0.864+-0.033, train macro-f1 = 0.852+-0.038, test accuracy = 0.79+-0.022, test weighted-f1 = 0.786+-0.025, test macro-f1 = 0.761+-0.029\n",
      "Reading: 训练/path.csv\n",
      "Reading: 训练/cell.csv\n",
      "-1\n",
      "-1\n",
      "Second Model Training Started\n",
      "Combination 2 ['path', 'cell'] >  selected parameters = {'n_iter_no_change': 100, 'max_iter': 1500, 'learning_rate_init': 0.001, 'hidden_layer_sizes': (64, 32)}, train accuracy = 0.846+-0.019, train weighted-f1 = 0.844+-0.021, train macro-f1 = 0.828+-0.024, test accuracy = 0.754+-0.019, test weighted-f1 = 0.752+-0.021, test macro-f1 = 0.724+-0.029\n"
     ]
    }
   ],
   "source": [
    "for trials in range(len(trial_combs)):\n",
    "    node_networks2 = [node_networks[i] for i in trial_combs[trials]] # list(set(a) & set(feature_networks))\n",
    "    netw_base = node_networks2[0]\n",
    "    emb_file = save_path + 'Emb_' +  netw_base + '.pkl'\n",
    "    with open(emb_file, 'rb') as f:\n",
    "        #emb = pickle.load(f)\n",
    "        emb = CPU_Unpickler(f).load()\n",
    "    emb = emb.cpu()\n",
    "    if len(node_networks2) > 1:\n",
    "        for netw_base in node_networks2[1:]:\n",
    "            emb_file = save_path + 'Emb_' +  netw_base + '.pkl'\n",
    "            with open(emb_file, 'rb') as f:\n",
    "                #emb = pickle.load(f)\n",
    "                cur_emb = CPU_Unpickler(f).load()\n",
    "                cur_emb = cur_emb.cpu()\n",
    "            emb = torch.cat((emb, cur_emb), dim=1)\n",
    "    emb = emb.cpu()        \n",
    "    if addRawFeat == True:\n",
    "        is_first = 0\n",
    "        addFeatures = feature_networks_integration\n",
    "        for netw in addFeatures:\n",
    "            file = base_path + dataset_name + '/' + netw + '.csv'  # 假设您的特征文件为 netw.csv\n",
    "            print(\"Reading:\", file)\n",
    "            # 使用 pandas 读取 CSV 文件\n",
    "            feat_df = pd.read_csv(file)\n",
    "            # 如果需要将 DataFrame 转换为 NumPy 数组，可以使用 .values 或 .to_numpy()\n",
    "            feat = feat_df.values  # 将特征转换为 NumPy 数组\n",
    "            if is_first == 0:\n",
    "                allx = torch.tensor(feat, device=device).float()\n",
    "                is_first = 1\n",
    "            else:\n",
    "                allx = torch.cat((allx, torch.tensor(feat, device=device).float()), dim=1)   \n",
    "        \n",
    "        if optional_feat_selection == True:     \n",
    "            allx_flat = [item for sublist in allx.tolist() for item in sublist]\n",
    "            allx_temp = robjects.FloatVector(allx_flat)\n",
    "            robjects.globalenv['allx_matrix'] = robjects.r('matrix')(allx_temp)\n",
    "            robjects.globalenv['allx_x'] = robjects.IntVector(allx.shape)\n",
    "            robjects.globalenv['labels_vector'] = robjects.IntVector(labels.tolist())\n",
    "            robjects.globalenv['top'] = boruta_top_features\n",
    "            robjects.globalenv['maxBorutaRuns'] = boruta_runs\n",
    "            robjects.r('''\n",
    "                require(rFerns)\n",
    "                require(Boruta)\n",
    "                labels_vector = as.factor(labels_vector)\n",
    "                allx_matrix <- Reshape(allx_matrix, allx_x[1])\n",
    "                allx_data = data.frame(allx_matrix)\n",
    "                colnames(allx_data) <- 1:allx_x[2]\n",
    "                allx_data <- allx_data %>%\n",
    "                    mutate('Labels' = labels_vector)\n",
    "                boruta.train <- Boruta(allx_data$Labels ~ ., data= allx_data, doTrace = 0, getImp=getImpFerns, holdHistory = T, maxRuns = maxBorutaRuns)\n",
    "                thr = sort(attStats(boruta.train)$medianImp, decreasing = T)[top]\n",
    "                boruta_signif = rownames(attStats(boruta.train)[attStats(boruta.train)$medianImp >= thr,])\n",
    "                    ''')\n",
    "            boruta_signif = robjects.globalenv['boruta_signif']\n",
    "            robjects.r.rm(\"allx_matrix\")\n",
    "            robjects.r.rm(\"labels_vector\")\n",
    "            robjects.r.rm(\"allx_data\")\n",
    "            robjects.r.rm(\"boruta_signif\")\n",
    "            robjects.r.rm(\"thr\")\n",
    "            topx = []\n",
    "            for index in boruta_signif:\n",
    "                t_index=re.sub(\"`\",\"\",index)\n",
    "                topx.append((np.array(allx).T)[int(t_index)-1])\n",
    "            topx = np.array(topx)\n",
    "            emb = torch.cat((emb, torch.tensor(topx.T, device=device)), dim=1)\n",
    "            print('Top ' + str(boruta_top_features) + \" features have been selected.\")\n",
    "        else:\n",
    "            print(emb.get_device())\n",
    "            print(allx.get_device())\n",
    "            emb = torch.cat((emb, allx), dim=1)\n",
    "    labels_tensor = torch.tensor(labels, device=device)  # 将 labels 转换为张量\n",
    "\n",
    "    data = Data(x=emb, y=labels_tensor)  # 使用张量创建 Data 对象\n",
    "\n",
    "    #data = Data(x=emb, y=labels)\n",
    "    \n",
    "    data.cpu()\n",
    "    train_mask = np.array([i in set(train_valid_idx) for i in range(data.x.shape[0])])\n",
    "    data.train_mask = torch.tensor(train_mask, device=device)\n",
    "    test_mask = np.array([i in set(test_idx) for i in range(data.x.shape[0])])\n",
    "    data.test_mask = torch.tensor(test_mask, device=device)\n",
    "    X_train = pd.DataFrame(data.x[data.train_mask].numpy())\n",
    "    X_test = pd.DataFrame(data.x[data.test_mask].numpy())\n",
    "    y_train = pd.DataFrame(data.y[data.train_mask].numpy()).values.ravel()\n",
    "    y_test = pd.DataFrame(data.y[data.test_mask].numpy()).values.ravel()\n",
    "    print(\"Second Model Training Started\")\n",
    "\n",
    "    if int_method == 'MLP':\n",
    "        params = {'hidden_layer_sizes': [ (64, 32)],\n",
    "                  'learning_rate_init': [0.001],\n",
    "                  'max_iter': [ 1500],\n",
    "                  'n_iter_no_change': [100]}\n",
    "        search = RandomizedSearchCV(estimator = MLPClassifier(solver = 'adam', activation = 'relu', early_stopping = True), \n",
    "                                    return_train_score = True, scoring = 'f1_macro', \n",
    "                                    param_distributions = params, cv = 4, n_iter = xtimes, verbose = 0)\n",
    "        search.fit(X_train, y_train)\n",
    "        model = MLPClassifier(solver = 'adam', activation = 'relu', early_stopping = True,\n",
    "                              max_iter = search.best_params_['max_iter'], \n",
    "                              n_iter_no_change = search.best_params_['n_iter_no_change'],\n",
    "                              hidden_layer_sizes = search.best_params_['hidden_layer_sizes'],\n",
    "                              learning_rate_init = search.best_params_['learning_rate_init'])\n",
    "        \n",
    "    elif int_method == 'XGBoost':\n",
    "        params = {'reg_alpha':range(0,10,1), 'reg_lambda':range(1,10,1) ,'max_depth': range(1,6,1), \n",
    "                  'min_child_weight': range(1,10,1), 'gamma': range(0,6,1),\n",
    "                  'learning_rate':[0, 1e-5, 0.0001, 0.001, 0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 1],\n",
    "                  'max_delta_step': range(0,10,1), 'colsample_bytree': [0.5, 0.7, 1.0],\n",
    "                  'colsample_bylevel': [0.5, 0.7, 1.0], 'colsample_bynode': [0.5, 0.7, 1.0]}\n",
    "        fit_params = {'early_stopping_rounds': 10,\n",
    "                     'eval_metric': 'mlogloss',\n",
    "                     'eval_set': [(X_train, y_train)]}\n",
    "        \n",
    "              \n",
    "        search = RandomizedSearchCV(estimator = XGBClassifier(use_label_encoder=False, n_estimators = 1000, \n",
    "                                                                  fit_params = fit_params, objective=\"multi:softprob\", eval_metric = \"mlogloss\", \n",
    "                                                                  verbosity = 0), return_train_score = True, scoring = 'f1_macro',\n",
    "                                        param_distributions = params, cv = 4, n_iter = xtimes, verbose = 0)\n",
    "        \n",
    "        search.fit(X_train, y_train)\n",
    "        \n",
    "        model = XGBClassifier(use_label_encoder=False, objective=\"multi:softprob\", eval_metric = \"mlogloss\", verbosity = 0,\n",
    "                              n_estimators = 1000, fit_params = fit_params,\n",
    "                              reg_alpha = search.best_params_['reg_alpha'],\n",
    "                              reg_lambda = search.best_params_['reg_lambda'],\n",
    "                              max_depth = search.best_params_['max_depth'],\n",
    "                              min_child_weight = search.best_params_['min_child_weight'],\n",
    "                              gamma = search.best_params_['gamma'],\n",
    "                              learning_rate = search.best_params_['learning_rate'],\n",
    "                              max_delta_step = search.best_params_['max_delta_step'],\n",
    "                              colsample_bytree = search.best_params_['colsample_bytree'],\n",
    "                              colsample_bylevel = search.best_params_['colsample_bylevel'],\n",
    "                              colsample_bynode = search.best_params_['colsample_bynode'])\n",
    "                            \n",
    "    elif int_method == 'RF':\n",
    "        max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "        max_depth.append(None)\n",
    "        params = {'n_estimators': [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)],\n",
    "                  'max_depth': max_depth,\n",
    "                  'min_samples_split': [2, 5, 7, 10],\n",
    "                  'min_samples_leaf': [1, 2, 5, 7, 10], \n",
    "                 'min_impurity_decrease':[0,0.5, 0.7, 1, 5, 10],\n",
    "                 'max_leaf_nodes': [None, 5, 10, 20]}\n",
    "        \n",
    "        search = RandomizedSearchCV(estimator = RandomForestClassifier(), return_train_score = True,\n",
    "                                    scoring = 'f1_macro', param_distributions = params, cv=4,  n_iter = xtimes, verbose = 0)\n",
    "        \n",
    "        search.fit(X_train, y_train)\n",
    "        model=RandomForestClassifier(n_estimators = search.best_params_['n_estimators'],\n",
    "                                     max_depth = search.best_params_['max_depth'],\n",
    "                                     min_samples_split = search.best_params_['min_samples_split'],\n",
    "                                     min_samples_leaf = search.best_params_['min_samples_leaf'],\n",
    "                                     min_impurity_decrease = search.best_params_['min_impurity_decrease'],\n",
    "                                     max_leaf_nodes = search.best_params_['max_leaf_nodes'])\n",
    "\n",
    "    elif int_method == 'SVM':\n",
    "        params = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "                  'gamma': [1, 0.1, 0.01, 0.001, 0.0001, 'scale', 'auto'],\n",
    "                  'kernel': ['linear', 'rbf']}\n",
    "        \n",
    "        search = RandomizedSearchCV(SVC(), return_train_score = True,\n",
    "                                    scoring = 'f1_macro', param_distributions = params, cv=4, n_iter = xtimes, verbose = 0)\n",
    "        \n",
    "        search.fit(X_train, y_train)\n",
    "        model = SVC(kernel=search.best_params_['kernel'],\n",
    "                  C = search.best_params_['C'],\n",
    "                  gamma = search.best_params_['gamma'])\n",
    "\n",
    " \n",
    "    av_result_acc = list()\n",
    "    av_result_wf1 = list()\n",
    "    av_result_mf1 = list()\n",
    "    av_tr_result_acc = list()\n",
    "    av_tr_result_wf1 = list()\n",
    "    av_tr_result_mf1 = list()\n",
    " \n",
    "        \n",
    "    for ii in range(xtimes2):\n",
    "        model.fit(X_train,y_train)\n",
    "        predictions = model.predict(X_test)\n",
    "        y_pred = [round(value) for value in predictions]\n",
    "        preds = model.predict(pd.DataFrame(data.x.numpy()))\n",
    "        av_result_acc.append(round(accuracy_score(y_test, y_pred), 3))\n",
    "        av_result_wf1.append(round(f1_score(y_test, y_pred, average='weighted'), 3))\n",
    "        av_result_mf1.append(round(f1_score(y_test, y_pred, average='macro'), 3))\n",
    "        tr_predictions = model.predict(X_train)\n",
    "        tr_pred = [round(value) for value in tr_predictions]\n",
    "        av_tr_result_acc.append(round(accuracy_score(y_train, tr_pred), 3))\n",
    "        av_tr_result_wf1.append(round(f1_score(y_train, tr_pred, average='weighted'), 3))\n",
    "        av_tr_result_mf1.append(round(f1_score(y_train, tr_pred, average='macro'), 3))\n",
    "        \n",
    "    if xtimes2 == 1:\n",
    "        av_result_acc.append(round(accuracy_score(y_test, y_pred), 3))\n",
    "        av_result_wf1.append(round(f1_score(y_test, y_pred, average='weighted'), 3))\n",
    "        av_result_mf1.append(round(f1_score(y_test, y_pred, average='macro'), 3))\n",
    "        av_tr_result_acc.append(round(accuracy_score(y_train, tr_pred), 3))\n",
    "        av_tr_result_wf1.append(round(f1_score(y_train, tr_pred, average='weighted'), 3))\n",
    "        av_tr_result_mf1.append(round(f1_score(y_train, tr_pred, average='macro'), 3))\n",
    "        \n",
    "\n",
    "    result_acc = str(round(statistics.median(av_result_acc), 3)) + '+-' + str(round(statistics.stdev(av_result_acc), 3))\n",
    "    result_wf1 = str(round(statistics.median(av_result_wf1), 3)) + '+-' + str(round(statistics.stdev(av_result_wf1), 3))\n",
    "    result_mf1 = str(round(statistics.median(av_result_mf1), 3)) + '+-' + str(round(statistics.stdev(av_result_mf1), 3))\n",
    "    tr_result_acc = str(round(statistics.median(av_tr_result_acc), 3)) + '+-' + str(round(statistics.stdev(av_tr_result_acc), 3))\n",
    "    tr_result_wf1 = str(round(statistics.median(av_tr_result_wf1), 3)) + '+-' + str(round(statistics.stdev(av_tr_result_wf1), 3))\n",
    "    tr_result_mf1 = str(round(statistics.median(av_tr_result_mf1), 3)) + '+-' + str(round(statistics.stdev(av_tr_result_mf1), 3))\n",
    "    \n",
    "    \n",
    "    # 创建一个空的 DataFrame，只需在外部定义一次\n",
    "    if 'df' not in locals():  # 检查 df 是否已经定义\n",
    "        df = pd.DataFrame(columns=['Comb No', 'Used Embeddings', 'Added Raw Features', 'Selected Params', 'Train Acc', 'Train wF1', 'Train mF1', 'Test Acc', 'Test wF1', 'Test mF1'])\n",
    "\n",
    "    # 创建一个新的 Series\n",
    "    x = [trials, node_networks2, addFeatures, search.best_params_, \n",
    "        tr_result_acc, tr_result_wf1, tr_result_mf1, result_acc, result_wf1, result_mf1]\n",
    "    new_row = pd.Series(x, index=df.columns)\n",
    "\n",
    "    # 使用 pd.concat() 添加新行\n",
    "    df = pd.concat([df, new_row.to_frame().T], ignore_index=True)\n",
    "    \n",
    "    print('Combination ' + str(trials) + ' ' + str(node_networks2) + ' >  selected parameters = ' + str(search.best_params_) + \n",
    "      ', train accuracy = ' + str(tr_result_acc) + ', train weighted-f1 = ' + str(tr_result_wf1) +\n",
    "      ', train macro-f1 = ' +str(tr_result_mf1) + ', test accuracy = ' + str(result_acc) + \n",
    "      ', test weighted-f1 = ' + str(result_wf1) +', test macro-f1 = ' +str(result_mf1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2847,  0.2172, -0.0086,  ...,  0.1560, -0.0613,  0.1376],\n",
      "        [ 0.3201,  0.2804, -0.0451,  ...,  0.1914,  0.0169,  0.1848],\n",
      "        [ 0.2479,  0.2541, -0.0144,  ...,  0.1752,  0.0586,  0.1990],\n",
      "        ...,\n",
      "        [ 0.0554,  0.1556, -0.1033,  ...,  0.2042,  0.1142,  0.2015],\n",
      "        [ 0.0430,  0.1774, -0.1176,  ...,  0.1959,  0.1647,  0.2079],\n",
      "        [ 0.0896,  0.1983, -0.1619,  ...,  0.1934,  0.1689,  0.2400]])\n"
     ]
    }
   ],
   "source": [
    "print(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data exported successfully to test_results.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 假设 test_idx, y_pred 和 X_test 是您的数据\n",
    "# 将它们转换为 DataFrame\n",
    "# 如果 test_idx 是一个列表或数组\n",
    "test_idx_df = pd.DataFrame(test_idx, columns=['Test Index'])\n",
    "\n",
    "# 如果 y_pred 是一个列表或数组\n",
    "y_pred_df = pd.DataFrame(y_pred, columns=['Predictions'])\n",
    "\n",
    "# 如果 X_test 是一个 DataFrame 或者可以转换为 DataFrame\n",
    "y_test_df = pd.DataFrame(y_test)\n",
    "\n",
    "# 将所有数据合并到一个 DataFrame 中\n",
    "result_df = pd.concat([test_idx_df, y_pred_df, y_test_df], axis=1)\n",
    "\n",
    "# 导出为 CSV 文件\n",
    "result_df.to_csv('D:\\\\test_results.csv', index=False)\n",
    "\n",
    "print(\"Data exported successfully to test_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 182.5 seconds for all runs.\n",
      "MOGAT is done.\n",
      "Results are available at MOGAT_训练_results_1/MOGAT_results.xlsx\n"
     ]
    }
   ],
   "source": [
    "end = time.time()\n",
    "print('It took ' + str(round(end - start, 1)) + ' seconds for all runs.')\n",
    "print('MOGAT is done.')\n",
    "print('Results are available at ' + excel_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib  # 或者 import pickle\n",
    "\n",
    "# 假设 model 是组合2训练后的模型\n",
    "model_filename = 'model_combination_2.pkl'  # 指定保存的文件名\n",
    "joblib.dump(model, model_filename)  # 使用 joblib 保存模型\n",
    "torch.save(model,\"model_combination_2.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test shape: (832, 1453)\n",
      "       0         1         2         3         4         5         6     \\\n",
      "0  0.188567 -0.002946  0.259677  0.232879  0.299379  0.245001  0.315489   \n",
      "1  0.191661 -0.039185  0.246106  0.213106  0.305355  0.248259  0.299791   \n",
      "2  0.209665 -0.028581  0.229080  0.210698  0.302300  0.239930  0.289490   \n",
      "3  0.233885 -0.014957  0.244939  0.205171  0.298337  0.235393  0.289586   \n",
      "4  0.197028 -0.008709  0.262457  0.210052  0.287729  0.285531  0.319330   \n",
      "\n",
      "       7         8         9     ...      1443      1444      1445      1446  \\\n",
      "0  0.180616  0.098148 -0.055966  ... -1.129061  0.135429  0.135784 -0.008405   \n",
      "1  0.147041  0.089811 -0.082131  ... -1.091605  0.185639  0.113645 -0.013304   \n",
      "2  0.161417  0.079199 -0.077170  ... -1.093004  0.069215  0.133506 -0.041868   \n",
      "3  0.143058  0.087553 -0.052084  ... -1.051855  0.063913  0.041288 -0.008137   \n",
      "4  0.149408  0.094126 -0.053295  ... -1.109466  0.178767  0.134429 -0.024522   \n",
      "\n",
      "       1447      1448      1449      1450      1451      1452  \n",
      "0  0.126665  0.129989 -0.010731 -0.849694  0.204449  0.206245  \n",
      "1  0.175527  0.137006  0.013353 -0.808921  0.186972  0.194370  \n",
      "2  0.024458  0.097858  0.036040 -0.863158  0.181604  0.177252  \n",
      "3  0.075392  0.170244  0.048130 -0.761205  0.212192  0.188098  \n",
      "4  0.187820  0.151531  0.078483 -0.830192  0.255994  0.269565  \n",
      "\n",
      "[5 rows x 1453 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 假设三个 CSV 文件的路径\n",
    "file1 = '外部测试\\\\path.csv'\n",
    "file2 = '外部测试\\\\cell.csv'\n",
    "file3 = 'MOGAT_训练_results_1\\\\Output_path.csv'\n",
    "file4='MOGAT_训练_results_1\\\\Output_cell.csv'\n",
    "# 读取 CSV 文件\n",
    "df1 = pd.read_csv(file1)\n",
    "df2 = pd.read_csv(file2)\n",
    "df3 = pd.read_csv(file3)\n",
    "df4 = pd.read_csv(file4)\n",
    "# 将三个 DataFrame 的列整合到一起\n",
    "X_test = pd.concat([df1, df2, df3,df4], axis=1)\n",
    "X_test.columns = range(1453)\n",
    "# 输出整合后的 X_test 的形状和内容\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(X_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: 外部测试\\labels.csv\n",
      "MOGAT is running..\n"
     ]
    }
   ],
   "source": [
    "path = base_path  + dataset_name\n",
    "if not os.path.exists(path):\n",
    "    raise FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT), path)\n",
    "        \n",
    "device = torch.device('cpu')\n",
    "#torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "#torch.cuda.set_device(6)\n",
    "\n",
    "\n",
    "data_path_node =  base_path + dataset_name +'/'\n",
    "run_name = 'MOGAT_'+  dataset_name + '_results_1'\n",
    "save_path = base_path + run_name + '/'\n",
    "excel_file = save_path + \"MOGAT_results.xlsx\"\n",
    "\n",
    "if not os.path.exists(base_path + run_name):\n",
    "    os.makedirs(base_path + run_name + '/')\n",
    "\n",
    "# 读取 CSV 文件\n",
    "file = '外部测试\\\\labels.csv'  # 假设您的标签文件为 labels.csv\n",
    "print(\"Reading:\", file)\n",
    "# 使用 pandas 读取 CSV 文件\n",
    "labels_df = pd.read_csv(file)\n",
    "# 假设标签在 CSV 文件中的一列名为 'label'，您可以根据实际情况调整列名\n",
    "labels = labels_df['diagnosis'].values  # 将标签转换为 NumPy 数组\n",
    "\n",
    "file = base_path + 'data/' + dataset_name + '/mask_values.pkl'\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "is_first = 0\n",
    "\n",
    "print('MOGAT is running..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "# 2. 加载保存的模型\n",
    "model_filename = '训练\\\\model_combination_2.pkl'  # 指定保存的文件名\n",
    "model = joblib.load(model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_tensor = torch.tensor(labels, device=device)  # 将 labels 转换为张量\n",
    "\n",
    "data = Data(x=X_test, y=labels_tensor)  # 使用张量创建 Data 对象\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.DataFrame(data.x)\n",
    "y_test = pd.DataFrame(data.y).values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 使用训练好的模型进行预测\n",
    "predictions_external = model.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictions_external = pd.DataFrame(predictions_external)\n",
    "result_df = pd.concat([labels_df,predictions_external], axis=1)\n",
    "result_df.to_csv('D:\\\\test_results.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
